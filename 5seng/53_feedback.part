\section{Communication in Programming}

In this section, I address several different subjects spanning various practices in programming that emphasize - or consist entirely of - social practices and communication protocols. These are similar to the software engineering concepts from the prior sections: they are (overwhelmingly, though not categorically) not the terms that are used by the participants, unless they are explicitly incorporating what they see as useful software engineering lingo. An outside observer perspective can interpret both the presence and the absence of these practices, depending on how strict the interpretation and how much formality is desired. These concepts are arranged in a loosely-chronological order relative to a piece of code being created. These particular ones were included because all were present for at least two groups in the \emph{informal} variant. Their more formal (of varying degrees of increased formality) variant was at least the subject of discussion - if not implementation - for at least one group. Unlike issues in the prior two sections, these ``software engineering interventions'' are even farther away in terms of obvious need: it is unclear what the costs and benefits are, and whether the solution is well-suited to the problem at hand.

%In the earlier section on formats and data structures, 

There is a pattern of development where there is a ``working environment'' corresponding to \emph{exploration} and one corresponding to \emph{replication} of now-reliable (or reliable-enough) code. In the former, the scientist looks at a split-screen of script and execution environment (in MATLAB).  In the case of exploratory analysis, such as in the iPythonNotebook environment, the trouble is that at the point where a test case can be articulated, half the battle has been won. There is a lot that still has to happen before that. A lot of testing is basically ``sanity checking'' which is necessary because everything is so brittle and unstandardized that there is no sense of trust or faith in the code. This is a persistent, daily paying of the ``technical debt.''

\subsection{Design, Maintenance and Refactoring}

%Design patterns are distinct from architecture patterns, and from principles/mantras which are more 
In listing best practices for scientific computing, Wilson et al suggest to ``document design, not mechanics'' \cite{wilson2014best}. However, in their survey of claims about software engineering practices in scientific development, Heaton and Carver (2015) point out that design is not seen as a distinct activity in scientific development, especially modeling. In other words, the code disappears relative to the underlying math and scientific exploration. This increases the difficulty of documenting the design of code as a distinct (and distinctly error-prone) layer. More formal design would involve making charts and illustrations of the software, not just of the math (ie, equations) or the hypotheses (eg, whiteboard squiggles of depth profiles).
%Add bit about whiteboard drawing here
%TODO maintenance and refactoring

\subsection{Documentation}

When it comes to contextualized, persistence over time, and incorporating multiple research products into a coherent statement, email wins out over inline comments, and over API-style documentation\footnote{for example, doctrings in Python. Correctly formatted, these come up as tooltips when you type names of your own functions later.}, so it is no surprise that it is a much more pervasive form of documentation. Modular and inline documentation requires that the users be able to not only call up this documentation in their working environment, but also have recognized that this particular documentation is what is necessary. Although modular documentation supports larger codebases with more contributors, it demands continual discussion of its relationship to other forms of documentation and non-code artifacts in the research context. Furthermore, an email remains ``frozen in time'' in a way that a collaborative codebase does not; as a result, documentation, if it exists in code, assumes an ephemeral and brittle quality. Whereas the practice for searching an email inbox are now near-universal, the same is not (yet?) true of version-controlled histories.

\subsection{Debugging}

This was partially noted in the data-formats-and-structures section prior. In Chapter 2, reviewing literature, on the section about software engineering in sciences, much literature stresses a lack of design and debugging as distinct steps in programming, and on the collapse of the code and the thing it represents (the math) into one entity that sometimes works and sometimes does not. A few of the participants used a debugger, mainly those with a computer science background. An oceanographer who was actively trying to incorporate a debugger into her FORTRAN environment ultimately was not able to find a better solution than print-statement debugging, which was by far the most common type (aside from debugging-by-making-charts) and is addressed in the prior data-formats-and-structures section.

Debugging of some form is unavoidable, and there is not a well agreed upon single "best practice" of particular style of formal debugging that is widely enough adopted. I have included it for two reasons. First, an overarching claim in this section is the idea that each of the programming practices listed is inevitable in even the most ad-hoc programming practice. Second, all these practices are varied, some more than most. Ultimately, I include it in this list because of the claim developed in the prior paragraphs on Documentation and Design: in the scientific context, it is possible (and common) to do code work without recognizing code as a distinct element from the science; this results in particularly informal debugging (and testing) practices. And as I mentioned before this is not the term that participants used themselves.

Formal methods of debugging include: (1) using debuggers in the development environment for runtime debugging; (2) navigating between declaration and usage of a variable or function in the development environment, in addition to the step-through in a debugging-mode execution; and (3) pre-emptively anticipating possible violations of assumptions with descriptive exceptions and errors in the code to communicate the state at which a failure occurs when it does, rather than iteratively adding print statements to backtrack the error after it occurs. In the particular groups studied, these practices were adopted minimally if at all, paling in comparison by frequency of use to rapid visual examination of a graphical/chart output (which is debugging the science and code combined, not the code by itself).

The first two debugging methods require a more customized and complex development environment than any of those I saw. In the cases these practices were used, the scientists specifically said that it was fortunate that they were granted the necessary time (a few days to a week) to get "set up" with a good working environment. In all these cases, a trusted tech advisor helped to set up the right environment. Even the most seemingly-basic aspects of an effective programming environment, such as syntax highlighting, require active modification of an unfamiliar tool (eg, shell or emacs) - the difficulty of this is palpable in the first half day of each of the SWC-like workshops observed.

The last method - using exceptions and errors to allow communication, either between the program and its user, or between the programmer and her future confused self - is challenging specifically because it requires approaching code on its own terms, not the terms more familiar to the researcher (ie, the math or science).

\subsection{Verification and Validation}

Although software is rarely recognized as separable from the mathematical or analytical concepts it embodies, it is still subjected to fierce, continual scrutiny (which is also reported by Easterbrook and Johns in a study of climate modelers \cite{easterbrook2009engineering}). Heaton and Carver draw analogy between software engineers and code and scientific programmers and math to bridge the terminology of validation: ``Software engineers typically understand Validation as the process of ensuring that the project specification (and overall end product) matches the project goals or user requirements. In scientific software, this same concept is described as ensuring that the mathematical model (the equivalent of the project specification) matches the real world (the equivalent of the project goals or user requirements)'' \cite{heaton2015claims}.

\subsection{Testing}

Formally, testing refers to a suite of declarations of what output a function should produce wen given a certain input. Tests are used to check that code that is undergoing changes or additions remains correct. In the Agile programming paradigm, for example, the tests are supposed to be written before even the code, as this paves way for concrete and measurable goals. Many frameworks exist to automate tests, with manual testing sometimes necessary but ideally avoided to the extent possible. Software engineers distinguish different kinds of tests, such as unit tests (does this function, given the correct range of input, produces the desired output?), integration tests (do the different components of the complex program fit together?), and validation tests (does the entire complex artifact perform well relative to the initial specifications?). Informally, ``testing'' is that series of actions that ``convinces'' the scientist that her code works and that she can move on to using it in answering a scientific question.

Generally, the ``informal'' variant of testing involves generating plots and charts that demonstrate that ``it works.'' However, if we focus on the lack of software engineering distinctions (unit versus integration) and the ``manual'' nature of looking at charts \emph{we easily overlook} that such testing can (1) embody rigorous distinctions (boundary conditions, comparisons to different sources of data versus assumptions and self-checked math) and (2) expertly combine the human cognitive capacity to quickly interpret well-designed graphics with the need to operate in an area of great uncertainty, where things that are unexpected are either errors or discoveries, and if you knew on the outset how to tell them apart, you would have done this already.

One of the four groups studied began to write formal tests toward the end of the study; their adoption of testing is described in Chapter 7 as one of the illustrative case studies.

\subsection{Pair Programming and Code Review}

The term ``pair programming'' was occasionally noted but not really owned by anyone, though I witnessed almost everyone engage in the hallmark behaviors of pair programming: one programmer, ``driver,'' sits at the machine, while the other sits without touching the keyboard but asking a lot of questions. This process forces both programmers to articulate their assumptions and implicit knowledge, and this type of pair exchange was very common both among peers and between the more junior (the ``driver'') and the more senior (the question-asked) researchers. (This is distinct from, and complementary to, the dynamic where the more junior researcher watches as the more senior one demonstrates how to do something, and is at the keyboard is that something pertains to code.) When I was giving the exiting-the-field talks, where I had summarized some of the intermediate forms of my ideas on formal versus informal software engineering practices, and I pointed out in a brief aside that many of  the interactions I witnessed can be seen as pair programming, one of the (more senior) oceanographers responded by rejecting the term, not combative of me but in a move similar to that in ``I am not a real programmer.'' Not a declaration of wanting to be a programmer, but the opposite, a distancing.

Consider the following situation, where a junior student was working on an analysis script. The script is not run in full; parts of it are highlighted and executed, simulating a modularity that is not strictly part of the script design. She is using MATLAB. A section is executed that modifies values used by another section; the execution record is convoluted and when the resulting chart shows an unexpected peak or trend, the student is stumped. This is not her code; she is nervous to have me here and by way of explanation she mutters ``I don't know what [another, more senior student] was doing here.'' Her trouble understanding the code is superimposed on her trouble understanding the intention of the original writer of the code. Eventually she is stumped enough to fetch a senior member of the lab, George. He comes to help, sitting behind her and asking her questions like ``what does [this variable] do? where is it set? where does it change?'' Repeatedly, she generates charts and he asks her to interpret them, to spot the errors. They would not call this ``pair programming,'' because there are other components to this interaction: the more senior researcher is mentoring the more junior one in a way of thinking; there is a component of training by example, and of verification using visual objects that was previously noted.

More formal code review refers to systematic walk-through and feedback; this might be in a pair programming context, or through something like GitHub's annotation and commenting feature on commits that are submitted but not yet integrated into the overall codebase (pull requests). Formal code review is a mechanism that allows a contributor to the code only modify or add to the codebase with review from an individual appointed either by seniority (of skill) or expertise (in the particular element of the codebase) or (ideally) both. In GitHub, a contributor external to a public project may fork that project, and then request that the changes she made be merged into the codebase; this request triggers someone internal to the project to review the code, potentially making comments and requests for modification before approval may be granted. This is a very particular implementation of peer review, which is deeply ingrained in scientific work. Although no formal, Github-style code reviews took place during my study, people solicited and provided feedback on code, as well as associated a sense of authorship to pieces of code and their functionality: rather than frowning, ``this makes no sense,'' the scientist can just as well quip, ``I have no idea what Alice is doing here, I better ask her.''

The contexts which I studied were intertwined with teaching dynamics. ``Pair programming,'' to the extent that it took place during my observations, was a means for expertise exchange and, as in many other contexts, included a deliberate effort to help the more junior researcher or research assistant develop a sense for how to spot things that are strange; a taste for things that are interesting; and an intuition for what questions to ask to visually interrogate their data or model using a series of graphics.

The RegionalNowcast-Model group has a lot of practices in place, some of which include software engineering best practices, but many of which are organizational best practices that allow the scarce resources of the PI and their part-time technology advisor Ed. In Section 7.2, I include a story about a one-day ``sprint'' that the group hosted for themselves, and the annual SWC events they hosted for their department. Communication around code included other teaching and scientific-feedback patterns, such as ``show and tell.'' This dynamics was apparent in many meetings (some of which were actually called ``show and tell'' meetings, hence my use of the label to describe a common occurrence more broadly.)