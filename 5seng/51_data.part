\section{Where to Put What}

%\kt{TODO reduce occurreces of wisdom of hindsight}

%The chicken-and-egg problem of adoption is the most apparent in the technical debt incurred by ad-hoc formatting decisions that end up requiring downstream data cleaning. The specialized parser is necessary because the data from a third-party site, or retrieved ``hot off the press'' via professional direct correspondence; the 

In this section, I bring together a variety of concerns about data representation on disk(s) and in memory, which are coherent in that they appear in group discussions of \textit{where} to put \textit{what}, which concerns a variety of related concepts. \textbf{Meta-data} refers to the information that provides the necessary detail for reuse, which is included in a standard, shared \textbf{data format}; Zimmerman, for example, identifies other means, besides information embedded into data formats that data is made reusable \cite{zimmerman2007not}. \textit{Structured data,} particularly in database management systems (DBMSs) that abstract representation and programmatic handling (e.g., using a query language) from storage on disk, are not particularly common in this setting, though they are used by the CustomInstrument-Lab. However, in addition to more philosophical issues of how data \textit{ought} to be represented or stored are the more banal issues of \textit{what to put where,} including things like parsing time-related strings, and grappling with small insidious errors as a result of misunderstood time-representation schemes.

This story begins at a meeting that had not yet started. It was an instance of a regular meeting between the members of the CustomInstrument-Lab. However, the collaborators, who were not working in this building and had to walk over, were running a few minutes late as usual, so everyone already here was casually hanging out and checking in with one another. It had been a few weeks since the last meeting. The joint meetings of this group tended to have the quality, relative to other meetings within the members of the group, of a slow re-boot. People in attendance worked to recapture the momentum or shared conversation of the previous time. Mallory, a post-doc, came in, sat in a chair, and asked: ``Who's coming? What's the agenda?'' There was no urgent quality to the question, and Andrew, one of the people who played a \textit{software advisor} role, noted in casual response: ``We made some changes to names of functions,'' going on to explain that ``No more files once you're done with [a necessary one-time processing step].'' This generated an air of excitement, as George summarized the good news: ``now we have a database!'' and Mallory described it as a ``conceptual shift.'' The shift, here, was not only in the \textit{where} but also the \textit{what}: a database is a layer of abstraction higher than a file-based system. 

The representation of data can be encoded programmatically (data structures), through common standards (formats), and informally through inline comments, emails, or post-it notes-to-self. In this section, I consider formatting (as in, how data is formatted from the vantage point of a user - such as UNIX timestamp versus a human-readable time and date with a day of the week included) and data structures (as in, how data are stored and manipulated at a conceptual abstract level). These are related concepts because both are at core about how human-readable dates, measurements, and parameters are in the process of both tuning a model and running an analysis. In Section 5.3, I explicitly address debugging and testing practices, but for now, let us focus on that inevitable step of programming where an execution has resulted in something unexpected and the scientist is trying to identify the source of error by repeatedly interrogating what a particular value is at a particular moment of execution.

One way to do this kind of ``sanity-checking'' is through visualization of the values, a practice I introduced in Chapter 3 and to which I will return at the end of this chapter. This strategy, however, covers far from the complete range of errors; on many occasions, scientists who code have to go digging through code. At this point, the question ``what is the value?'' is answered not through specialized and dense visualizations, but through printouts of data structures and through intermediate output (e.g., comma-separated value output). Even in the best of cases, this is a far less human-friendly format.

One of the most common sources of error, relevant to researchers of all levels across all four groups, is some error parsing timestamp formatted in a slightly unexpected way. This affects not only the data processing pipeline but also the semantic organization of data, as time-related information - as condensed as possible - is embedded in filenames and folder structures. Additionally, variations in representing time include what to do with null/zero values (dense versus sparse representation). At one point, in the CustomInstrument-Lab, the error was that a student who was making a chart assuming that the database with the data had no missing rows of this sort, and therefore could be aggregated (to make the ``big'' data more manageable given the specific research question) three adjacent rows at a time. However, there were gaps where collection had been stopped for technical reasons: so her assumption, having not gone on the cruise and been as familiar with the data as those who did, is understandable. But this resulted in obscuring a very interesting finding that was sensitive to daily patterns.

Or consider the following exchange in CustomInstrument-Lab team, between a software advisor, Andrew, and a post-doctoral research fellow, George. Andrew is explaining a recent fix to George: ``oh, I made it so file names are consistent - new function, [foo], which detects old vs new style and formats folder and file itself.'' George clarifies: ``so user doesn't have to do this?'' And Andrew confirms, which seems to make George quite happy. As they move on, George walks Andrew through a function adopting the familiar second-person voice (``and here you do [this] to [that]'') and Andrew looks over his shoulder. At a particular point, Andrew points out, incidentally, since the topic had shifted somewhat, a place where the \_ function is relevant. George laughs and asks: ``why is this happening \emph{now}? Didn't we fix it?'' Andrew explains, with George recasting his understanding in less technical terms to confirm that they are on the same page. Andrew sighs: ```I wish we never changed file naming convention...'' George does not let this remark slide unnoticed: ``oh no no no!'' but Andrew reassures: ``no, I mean, I wish we \emph{started} with time-stamp.'' George, reassured successfully, tells a quick side story about another researcher who insists on using an unreadable numerical time-stamp. Andrew agrees with the implicit verdict: ``it's good for computer, not good to access as a person,'' going on to explain that ``most libraries interpret correctly despite multiple standards for date formats,'' so there is really no advantage to the numerical representation.

``Technical debt'' is another term, like debugging, which was applied virtually not at all by the scientists to themselves, but which appears routinely in discussion of scientific programming by computing researchers and practitioners.  \ca{reference for technical debt????} Data formatting decisions constitute a visible form of technical debt: burdened by the inescapable wisdom of hindsight, the oceanographers observed expressed the most regret with regard to how data were stored and accessed. The frustration of time sunk into shuffling bits around inefficiently is very different from the experience of analyses scripts that may seem inefficient from the outside. If some thing takes an hour or two to run but could be re-written to be only 15 minutes, it might be not problematic at all in some cases: if the procedure is robust enough to execute without supervision, allowing the researcher to launch a process and eat lunch, go home, take a break, or transition to a different tasks. If there is a more efficient, faster version, it is only better if it also requires no supervision; if the faster script is also occasionally buggy, or somehow less certain in its correctness, it demands more researcher attention and time than the computationally-slower alternative. Inefficient data parsing, writing, and processing, however, always demands attention because it is fundamentally error-prone  because it is basically almost always a work-around an upstream problem: the original data format is subject to the peculiarities of the process and team that produced it. Even with standard accepted formats, like NetCDF \cite{rew1990netcdf}, some interpretation is necessary.

At one point, I was interviewing Melissa, a computer science student who at one point had been part of the CustomInstrument-Lab collaboration. Her involvement was winding down by the time I began my observations, so in interviews I mostly asked reflective questions. After one such interview, we continued the conversation and she tells me how she sent Argo data to some UW researchers working with Argo floats\footnote{``Part of the integrated global observation strategy,'' a project that now offers 15 years of global float data. See http://www.argo.ucsd.edu/}. Articulating her troubles finding a group of physical oceanographers to collaborate with, she told me: ``I just hoped they would be more excited, but they were getting caught up on things that I didn't think were important.'' In particular, she had sent them something she had made based on the data, and received in response an explanation for how a particular column ought not be used for data (it is only used for quality control). The column names had been named without enough verbosity in the NetCDF: the common format didn't save a capable and enthusiastic researcher from violating unstated assumptions. This was frustrating for her, because her intention to demonstrate a visual artifact process had backfired in a way that she felt was disproportionate to the difficulty of fixing the issue: ``this is a \emph{database}, we can filter that out later! I was \emph{just} plotting something!''

During a shadowing session with the RegionalNowcast-Model team, Erin spends some time looking at data published on the website of a particular service which makes available daily weather data, relevant for comparison against their short-term forecasts (nowcasts). Her goal is to ``figure out how to parse it into a format our models like to read...'' She continues: ``but before investing time in that, I would like to figure out if the data is good enough. This is a task I haven't been looking forward to doing - it's just not fun to parse data from files. The first step was to paste data from file into my [iPython] notebook, that was a waste of time, I've given up on that. [The idea was to see if I could] just look at it quickly without writing a parser? Not the way to go, can only look at one day, and the location I chose is not close to what I care about. So it was a big waste of time [to attempt to avoid writing a parser by processing data `manually']. Now, I will write [a script] to parse [the needed third-party data].''


Provenance information is indispensable,  but it may not be clear which parts are actually necessary to record until it becomes necessary. \ca{any refs?} The technical capabilities of the data storage format can be perfect but if the limitation is conceptual, will still bottleneck on emails to someone who was on the cruise. That said, the technical implementation is often far-from-perfect. The format that is ``simple'' and ``readable'' to produce can also be the one that generates more work for parsing. While talking to a junior (undergraduate-level) research assistant who was working on ``cleaning up'' some of the code, commenting that ``the hardest thing is to make [the output] user friendly'' and ``access to type of input someone could use.'' I ask if there are any standard formats, and at first she is confused but then she laughs: ``no. That would be nice, but not that I know of.''

On another occasion, Omics-Lab group is discussing in a meeting an especially interesting and relevant new paper with a cornucopia of data and figures in the appendix. However, the accessibility of something does not necessarily mean knowing enough about data provenance to use it; after spending a bit over an hour collectively discussing and interrogating the paper, its figures, and even opening the attached (partial) data in a spreadsheet, the conclusion is to contact some of the people in the team that published the paper to learn more how to work with this data. Ad-hoc, immediately readable formats are common for the same reason that inconsistent time-stamps are common. Using standard formats introduces a lot of structure, which is at best unnecessarily verbose and at worst brittle and easy to render totally useless with errors. However, intermediate output is vital to a particular form of debugging. Short outputs peculiar to the context and the error in question become beacons of sanity in a  sea of unpredictable fragility. %TODO more about nancys formats here, with transition like other than immediate needs upstream formatting decisions affect thigns

So far, I have provided examples of issues that come up relating to \emph{how values look to a programmer or user of analysis software}, not so much \emph{how values are represented computationally}. In a programming environment where the ``what is the variable's value now?'' question is primarily answered by printing out the variable to shell or out to a file which can be monitored as the operation progresses, the format and the structure are more connected than in the case with database management systems that provide more abstraction, or even with complex meta-data rich structures (extracted from standard files) which are impractical to print in full during debugging.

The separation and abstraction of a DBMS allows a certain degree of power but also shifts the locus of control and scrutiny; if it is beyond the scientist's programming skills, it is qualitatively different that being merely less efficient or less powerful. When the CustomInstrument-Lab experienced a data loss due to a technical issue with the software and hardware data infrastructure maintained by their computer science collaborators, though the data was not ``lost'' in the sense of ``gone for ever'' (the way that a water sample damaged in shipment might be lost), but it was rendered inaccessible enough that the scientist most affected by the end said she had spent an additional month trying to get the data back into the database. The person who left in the story ``The Departure'' in Chapter 3 was the person who had gotten the data in there; since he was not available, that work had to not only be carried out again, but carried out with a sense of failure and frustration.

%In fact, at one point during the exchange described in ``The Departure'' where David is leaving and George, in last-minute hastiness, asks him to help with some code. David  gets down on his knees in front of Francois’s computer, after asking George to ``show me your code... you have a database, it's a start.''  George  is scrolling through a very long file. David asks what a particular function does, George describes its input and output; take X, get Y. David asks, in an attempt to help speed-debug an issue: ``can you go to the \_ function, and make it print out the SQL and exit without the get db query?'' But when George offers him the chair, he says, ``no, no, I don't know R'' So as he tries to print out the intermediate query tries to figure out a date Dan: doesn’t matter [trying to get F to abstract out the pointless details] Francois: but what if it’s an inaccurate date? [meaning: not in a cruise] Dan: it’s just printing out, it doesn’t matter

%\footnote{Complexity when it comes to pushing bits around - cleaning, processing, re-formatting, etc data - is squarely undesirable. But what it simple? During the course of writing this document, I am occasionally distracted by a recent massive web development error where a trivial, 11-line function with the job of adding padded whitespace has become unavailable due to the decision of the open-source developer who wrote the package that included it unavailable.OK:[?] The trouble is that there is a teetering tower of dependencies that toppled as a result. In reading that, while taking a break from writing up my findings, my first thought is - the study participants are doing everything they can to minimize being subjected to such brittleness. There are so many other sources of challenge in the study of oceanography, which I tried to give some idea of in Chapter 2, that the use of code is conservative, rather than brazen, relative to externally-dependent, possibly-flaky components. Not to mention that dependency management is confusing and difficult even in the best-case scenarios - see ``The Upgrade'' in Chapter 3. (Example, invariably emotional and judgmental, press on the subject: http://www.haneycodes.net/npm-left-pad-have-we-forgotten-how-to-program/) }
The complexity and size of the database in the CustomInstrument-Lab group arose from the complexity and size of the the data themselves. A post doctoral fellow on the project, in an early interview\footnote{One of the ones from Spring 2014, conducted by Charlotte P. Lee's group, prior to the beginning of my study.}, identified ``the package that read raw [instrument] data, binary data, read them into something that makes sense'' as one of four R packages he uses in the course of the work. Indeed, refactoring this particular package was the first joint project between the oceanographers and the eScience researchers on campus. During my first observation with the team, I am given an introduction of the code, which is ``built in-house'' for 2 major analysis tasks: analysis during collection (on the cruise), and analysis afterwards (comparisons and longer-term storage). I am told by a senior researcher that they wrote some code 3 years ago, at which point others - who are also present in a small joint meeting regarding this software in particular - chime in with  jokes about how ``crappy'' that code was because it ``didn't involve any computer scientists'' and how now the computer scientists collaborators are helping to ``deal with a lot of bad programming.''

Other than the CustomInstrument-Lab, no other teams used a DBMS. At some point, as a particular meeting of the Omics-Lab wore on, it dawned on me that the participants in this particular meeting were using the word ``database'' to refer to a file. This was jarring to me: the term ``database'' overwhelmingly implied that the storage and manipulation of bits on disk is abstracted away from operations over the data. In this case, though, the textfiles contained sequences of interest, and were an embodiment of careful researcher-specific curation. Colleen notes that [colleague 1] is using [colleague 2]'s database, not GAWS, which surprises both the lab manager and the PI. Colleen is, in turn, surprised by their surprise: ``so should I use this dataset curated by him?'' to which the PI suggests: ``I think you should talk to [colleague].... this is like her life’s work, to make these databases, and she's right there.'' %[A] = Rachel TODO pseudonym

\kt{expand below?}
Study participants spent time and energy discussing issues of making their own data available for later and/or broader re-use. The data access endpoint can offer the following features, each of which can be noted above:
\begin{itemize}
    \item Online access, with permission restrictions (if any) automated. Removes the need for an email exchange to facilitate data access.
    \item Programmatic access, through enabling URL-hacking or an API. Removes the need for manual download of data. 
    \item Integration of multiple data sources and relational joins to allow users to download only that data which is necessary. Helps to alleviate problems of big-ness; the whole dataset may be unmanageable for a user, but perhaps the data actually needed is much smaller. Removes the user inheriting some of the "big data" problems. In practice, this can be achieved using a form with dropdowns or other UI elements, which may or may not co-exist with programmatic access in the above point.
    \item Surfacing of summary information about the data, including in visualizations. Removes the need for shot-in-the-dark trial-and-error (and/or emailing someone) in the above case.
    \item Timely access, relative to when data is collected: real-time from cruise, immediately after cruise, every 6 months when data is collected on a USB-drive from a particular location. The complexity of annotation and ingestion of data may improve its accessibility and usability, but it may reduce its timeliness.
\end{itemize}

% Too detailed for this chapter 
All these features are orthogonal, and the range of their various combinations are represented in the data endpoints for oceanographic data.  In addition to this plurality of mediated data access sites, increasingly common programming practices mediate the way in which these data are produced or manipulated. The examples in this section revolve around issues of \emph{where} and \emph{how} data is stored for computation. Ad-hoc, unstandardized formats arise from an immediate need for trouble-shooting: short, readable time-stamps at the right granularity are helpful for debugging, but not as helpful for moving forward. Because filenames and folder structures are used extensively for data management, and because \textit{time} is a key component (in addition to \textit{region}) both for analysis and the reality of data collection, decisions about time stamp readability affect decisions about file-management, and therefore data-management. Whether or not the word ``refactoring'' was used to describe the act of changing time-stamp, file naming convention\footnote{common and crucial for organization of data}, it was as unpleasant as it was necessary. It is not clear if using a database solves the conceptual problem here. A DBMS instance is far more brittle than a collection of files, which is relatively more understandable with simpler tools. The group that used a DBMS did it to dramatically extend their capabilities to work with more data. For analyses that did not require this resource, the scientists would occasionally do analysis on their local machines having downloaded the right view.

%The RegionalNowcast-Model group had put on, a second year in a row, an annual software carpentry workshop event targeted for researchers in earth and ocean sciences. Because the curriculum is already very dense and hard to get through in time, they excluded the relational databases unit, because the other units were more immediately relevant to the audience. During an early conversation with the SWC founder, Greg Wilson, he explained to me how each unit of the curriculum is intended to carry with it technical skills as well as conceptual ideas; in the case of databases, this is structured data. Are databases, then, sufficient or necessary for effective work with data?

%First, the examples from the CustomInstrument-Lab team suggest that having a well-designed DBMS instance is not sufficient. Second, the examples from the RegionalNowcast-Model team illustrate that file-based data storage can be an effective compromise between having a semantic structure (by way of folders and file-names) and not having too rigid a structure. In all the examples, there was minimal articulation by the scientists of the distinction between storage of data and its semantics; though this may be because I did not ask the right questions, it also is in line with prior work (in Chapter 2) and observations in debugging practice in general where there relatively little recognition of computing abstractions in the course of code work.OK:[I did not get prev point] In some cases, the inefficiency of dealing with the intermediate non-standard-formatted output is compensated by the relative ease of `sanity-checking' this allows.
%OK:[So? what is the bottom line? I guess u r still working on it I assume the bottome line in all would be to identify major stumbling blocks how to make it more efficient- its this right?]
%In this sense, I would suggest that data storage and manipulation schemes that make apparent this description might be necessary for 

%In the section on ``Biases and Researcher Stance'' in Chapter 3, I used the terms ``bug'' and ``debugging'' as an example of how code work in this setting can be interpreted. When code resulted in unexpected behavior, this was not (usually) termed a ``bug'' but rather ``the code is acting weird'' or ``the code doesn't like [some parameter value]'' or ``the [measures being estimated] are going to zero / going crazy.'' The use of a debugger was essentially nonexistent. The kids of challenges brought up in the prior section on data formats and structures may have particularly benefitted from a debugger.
%With SQLLIte running in terminal, Becca begins to explain tales, Andrew interrupts to say, “it’s basically like a data frame" then Becca gets back to explaining how to quit, how to get help. About something: “this is just how it is” #AcceptIdiosyncrasy
% Francois explaining at some point “I’m not an expert but maybe this means it’s not a proper data frame” he types “…it’s a function"
%Maria: “…why is it a function"
%He’s typing into the interactive R screen