\chapter{Background and Related Work}

In this chapter, I review background and related literature about what code in oceanography does and how it is made. In Section 1, I focus on issues relating to production of code in a scientific context. In Section 2, I offer a short summary of the study of the ocean, pointing out various loci of code work.

Studying anything about the ocean is difficult for many reasons. The ocean covers two thirds of the earth's surface. Regional and global patterns of movement are subject to the seasons, as well as major anthropogenic influences, such as ice melting and producing freshwater with a different density than its surroundings.  Organisms use up nutrients, die, and reproduce over the course of a day. Human activity, such as overfishing, influences the inhabitants of a complex dynamical system in a manner that is not only hard to interpret in term of future implications, but it is also hard to understand in the present. How many organisms of a particular kind are there? Even if they are big enough to count, they move all the time. What was the visibility underwater when the counting was done? How much of some nutrient or substance is there? Was it measured by submerging an instrument to a certain depth while at sea? How certain are we that the ship, in its movement, was not carrying  anything in its wake, or otherwise contaminating the findings? What is the shape of the ocean floor, the overall character of circulation patterns, the changes of salinity or temperature over the seasons and over time? How do you know your observations are reliable? How do you know your model represents something meaningful about the natural world? The study of the ocean is associated with many uncertainties are different levels. Skepticism and a plurality of complementary methods help make this uncertainty productive, rather than an insurmountable barrier.

Code work is part of this plurality of methods, integrated differently in different ocean science contexts. The place of software in this multi-disciplinary field has been both to enable computational modeling and to enable generating, handling, curating, and comparing increasing quantities of data.  Figure \ref{fig:poc} illustrates a simplification of the loci of ``code work'' in oceanography. Computational modeling allows scientists to make causal claims and formulate hypotheses which can be tested using observation. More robust use of database systems allows the possibility of integrating observational data. Greater scale, resolution, and integration of observational data strengthens its ability to be used as evidence in increasingly complex claims about how the ocean's biology, chemistry, and physics fit together into a coherent system. The coherent understanding of this world benefits from data stewardship efforts that make data more robustly available to scientists who would not otherwise have personal access to that data.

\section{Study Focus: Code Products and Code Work}

Borgman describes data as the ``glue'' of collaboration between scientists and technologists \cite{borgman2012whos}. Indeed, open data is a crucial means to integrate and accumulate knowledge \cite{reichman2011challenges} in a multidisciplinary environment, particularly one that is ``radically unstandardized'' \cite{steinhardt2015anticipation}. From the software engineering research side, particularly when it comes to numerical modelling traditions in the sciences, there is a ``lack of separation of concerns,''\footnote{Patel et al, in a study of programmers attempting to implement machine learning applications and having difficulty because they focus too much on algorithm selection and neglect data debugging \cite{patel2008examining}. This can be summarized as the opposite of the indictment that the scientists worry too much about their data and not enough about their code, leaving us with the unremarkable hypothesis that in problem-solving, people focus on what they know.} no sense of abstraction, where ``the problem to be solved entirely [is] mixed with numerical approaches and target-dependent information'' \cite{palyart2011improving}. Heaton and Carver note a similar claim in their systematic review of claims about software engineering practices in the sciences, specifically modeling \cite{heaton2015claims}. Sletholt, Hannay et al also find that ``the definition of test cases for validation and verification of the software is  perceived as challenging ... it is often not obvious to  stipulate whether an error lies within the scientific theory or in the  implementation (numerical approximation) of that very theory'' \cite{sletholt2011literature}. In this sense, the software can also be described as ``glue'' - once applied, inextricably and irreversibly linked to the other components. Or, as Ragan et al note in a Nature article on programming environments for microbial ecology: ``Although computation can be a language that bridges many disciplines, additional `glue' is often needed to make the requirements mutually comprehensible to diverse members of a project team'' \cite{ragan2013collaborative}. Here, the ``glue'' is more the kind of ``social glue'' that Henderson identifies in her work on engineering sketches,  where physical, flexible representations which are preferable to the more rigid technological environments because they act, by virtue of their flexibility, as a ``social glue'' between different communities of practice \cite{henderson1991flexible}.

The ``glue'' that holds this section together, in its wide-ranging review of half a dozen fields of scholarship, is the existence of scientists who take part in, or want to take part in, many different kinds of code work in the course of scientific craft. The study of adoption of tools is then linked to programming skill acquisition, which is in turn influenced by the context of the scientist and the extent to which certain kinds of code work are pervasive or rewarded. In the study of large collaborations (e.g., oceanography \cite{steinhardt2015anticipation} and radio astronomy \cite{paine2014producing}) or other contexts where there is relatively clear separation of skills and interests (e.g., data gathering at sea with robots \cite{borgman2012whos}), communication and collaboration practices are the focus of study. Outside of these massive projects, the principal investigators (PIs) are responsible for managing diverse groups with disparate skill-sets and backgrounds, for determining which innovations to invest (time, money, and energy) in, for creating additional roles within the group, and for advising the early-career members of that group not only on success within it but also beyond. This may be especially salient in disciplines like oceanography, where there are many different research methods and many different levels of associated collaborative work or major expenses, including software licenses, laboratory equipment, staff roles.

\subsection{Code Starts and Ends with Data}

A large body of work in computer science and adjacent fields (from information visualization to software engineering to computer-supported cooperative work) concerns itself with how scientists work with data as the fundamental building-block of empirical study of the natural world. In this section, I highlight scholarship that identifies ways in which code is deeply integrated with the production of data, as well as the dissemination of data in this study context.

In \emph{A Vast Machine}, Edwards writes: ``It is easy to forget that data are never an abstraction, never just ``out there.'' We speak of ``collecting'' data, as if they were apples or clams, but in fact we literally \textit{make} data'' \cite{edwards2010vast}. This quote is excerpted from a passage that describes how the act of digitizing an analog temperature reading is an act of constructing data. Data are \textit{co-produced} with code in science, as Paine and Lee find in an ethnographic study of a radio astronomy collaboration \cite{paine2014producing}. As environmental data is increasingly more available, instances of ``data avalanche'' in the 2000s \cite{howe2008end} or  ``explosion of data'' in the 1950s \cite{edwards2010vast} correspond to vocational shifts in what it means to ``be'' an ecologist or ``do'' ecology \cite{jackson2013infrastructure}.

Paine and Lee additionally write that the \textit{legitimacy} and \textit{importance} of code work among scientists who ``acknowledge that this work is necessary and a fact of their day-to-day lives, yet still do not entirely consider such work to truly be the `scientific' work in and of itself.'' Paine and Lee stress that ``iterative system development is key for answering their scientific questions and the development of the necessary research infrastructures'' and therefore that ``such `non-scientific' work has a deep and lasting impact on the group's scientific outcomes.'' % The issue is that "it isn't science is derogatory somehow

Baker et al introduce \emph{Ocean Informatics}, an area that inhabits intersection of oceanography, information sciences, and social sciences, drawing on the lessons learned from two independent programs with some common data management practices from over a decade \cite{baker2008enabling}. Below, I summarize the most salient points of the timelines of two programs from Baker et al's manuscript: Palmer Station and JGOFS.

In 1990, the program Palmer Station was established as a long-term ecological research (LTER) site, and the data was made available online in the form of static text. Given that the history of triangulating data and theory reaches back almost a century (\cite{kunzig2000mapping}) and the history of computational modeling over half a century (\cite{edwards2010vast}), the formalization of data access through an online endpoint is relatively recent. Nevertheless, it still predates Apache Subversion (founded 2000) and GitHub (founded 2008); it also predates the initial work on the iPython Notebook \cite{perez2012ipython}, which began in 2001 although the stable release came over a decade later. Aside from the point of precedent, I also want to highlight that because of the need for comparisons over time spans, time scales, and between geographical regions, long-term research sites are especially useful. This also means that data is available now about the era following the 1990s at scale and resolution that did not exist before that time, or exists in inaccessible format. The stewardship of \textit{historical} data is also important, not just forward-looking data infrastructures.% \kt{TODO want citation.}

Over the next decade, the capabilities of JGOFS illustrate that a data endpoint can be arbitrarily complex and feature-rich:

\litquote{baker2008enabling}{From its beginning in 1994, JGOFS had a ``Data Management Office'' with technical staff that worked together with investigators; ``much of the collaboration focused on issues related to quality control and the collection and subsequent publication of complete metadata for contributed data sets.'' ``All process study data were ingested into an object-oriented, relational database and made available via the World Wide Web. Using a standard Web browser client, users of the JGOFS data system can generate custom data sets that match their research interests by combining multiple data sources `on-the-fly'.'' In early 2000s, ``to meet requests for data queriability and requirements for networking,'' new system was designed featuring ``online data access, strategic integration and visualization.'' ``Data and metadata management is offered through web interfaces with tiered permissions that enable data provider participation in making their data accessible. The new system is built upon a relational database with an object-oriented API layer that supports Web-based data query.''}

The comparison of findings and predictions includes model output, which is addressed by the solution  in \cite{howe2008end} and ocean data view \cite{schlitzer2002interactive}, both of which focus on readjusting the grid and thereby enabling coherence between disparate data sources \cite{grochow2010client}. Enke et al, writing about barriers to biodiversity data sharing, also note the importance of combining observational and model data in data access endpoints \cite{enke2012user}. Baker et al write: ``As the JGOFS program transitioned from process-oriented field studies to modeling, the data system was extended... Synthesis and model results, larger in volume and often global [rather than regional] in scope... required a more graphically oriented user interface and extended visualization capabilities'' and the technical staff ``worked closely with investigators to provide timely availability of data during the active research phase and to ensure preservation of the completed data collection as an important part of the [program] legacy'' \cite{baker2008enabling}.


\subsection{Is code in scientific contexts special?}

In the previous section, I make the argument that code work is an important facet of scientific work that deserves study, particularly study that looks at code work as deeply integrated into other activities. In this section, I ask, what is distinct about software work in science relative to other contexts? Sletholt et al write: ``in contrast to the development of, say, administrative or business  enterprise software, the writers of scientific software cannot  determine what the correct output of an application should be in  the traditional sense. Also, the software may evolve through the  combined effort of a number of scientists over the course of many  years, continuously adding new functionality to the system \cite{sletholt2011literature}.'' This strand of research characterizes software production in the sciences as qualitatively different from other contexts because of a more uncertain concept of the end result or outcome  \cite{heaton2015claims}; requirements are volatile \cite{sletholt2011literature}; subject to continuous maintenance  \cite{sletholt2011literature}, following through with which is primarily socially-motivated (in a philosophical at least as much as a transactional sense) \cite{trainer2015personal}; not subject to formal design process \cite{heaton2015claims,paine2014producing} but nevertheless adapting various programming best-practices, most notably elements of the agile process \cite{heaton2015claims,sletholt2011literature}. In some cases, particularly in numerical approximation code, scientific code can exist without appropriate maintenance, relative to the high-performance computing hardware on which it is meant to be run \cite{palyart2011improving}. Heaton and Carver provide a systematic overview of studies of software engineering practices in scientific code (again, numerical approximation / modeling in particular), concluding also on the general lack of maintainability or readability of code \cite{heaton2015claims}.

Howison and Herbsleb note that ``Unlike other technologies supporting science, software can be copied and distributed at essentially no cost, potentially opening the door to unprecedented levels of sharing and collaborative innovation. Yet we do not have a clear picture of how software development for science fits into the day-to-day practice of science, or how well the methods and incentives of its production facilitate realization of this potential'' \cite{howison2011scientific}. This articulation of software practice as offering unprecedented potential is a crucial part of the vision of software in the perfect world as not only solving problems but fundamentally challenging the daily reality of work. Speaking to code in ecology in particular, Mislan et al note that ``even code that is rough and difficult to run on other systems (owing to software dependencies and differences in computing platforms) still provides valuable information as part of detailed documentation of the analyses'' \cite{mislan2016elevating}. The authors suggest some venues for publishing code, presenting different options side-by-side in a table ``Comparison of Common Resources,'' and urge journals to make code citation more prominent to address this issue. Trainer et al introduce the concept of ``extra work,'' referring to all the maintenance and support work that is not formally accommodated in scientific practice but which is practically necessary to make that code viable  \cite{trainer2015personal}. In their typology of motivations for doing said ``extra work,'' every category includes a social connection, an actual interpersonal request, or some other opportunity for recognition of effort or a sense that someone is actually using something.  Enke et al, writing about biodiversity \textit{data} sharing, divide all measures to encourage more sharing into ``stick'' and ``carrot'' measures, with many more specific implications on the basis of more than 700 survey responses \cite{enke2012user}, but adding to the chorus of needing recognition for doing thankless and crucial heavy lifting.


Free/Libre and Open Source Software (F/LOSS) is another form of software work that distinguishes itself from the ``typical'' industrial software setting \cite{leach2009freedom}. In a study of software ownership (``a general term used to describe whether one person has responsibility for a software component, or if there is no one clearly responsible developer''), Bird et al demonstrate and explore the claim that ``when there is no clear point of contact and the contributions to a software component are spread across many developers, there is an increased chance of communication breakdowns, misaligned goals, inconsistent interfaces and semantics, all leading to lower quality'' \cite{bird2011don}. They note that ``new methods ... profess collective code ownership but there has been little empirical evidence or backing of this data on reasonably mature/complex or large systems.'' Though not speaking of F/LOSS as a whole, they note one such paradigm of software practice which shares some practical components with the F/LOSS ethos described above and makes similar quality claims \cite{bird2011don}. Although F/LOSS distinguishes itself from the hierarchical industrial software setting, and vice versa, Leach comments also points out a surprisingly-pronounced hierarchy, with authority ``often enforced in displays of aggressive argument and belittling of others.'' Complementing its community-centric philosophy, F/LOSS is associated with a sense of individual craftsmanship, where ``the craft in question is satisfying only when it is seen in terms of a model of knowledge that locates agency and activity within the technologist, who thereby himself pushes back the frontier of knowledge.'' In this sense, ``morality and ethics are inseparable from the objects which the community produces. Politics, and an imagined future, are pursued through the construction and development of software, of objects, which themselves are to carry the burden of, and are believed to instantiate, an ethical and moral vision. Politics in this case can be engaged though writing software in a particular manner.'' \cite{leach2009freedom}

In their synthesis of literature on the challenges of software, especially communication, Bird et al draw a link between expertise and ownership: those who are responsible for the code have an expertise that is not merely helpful but necessary for future development on a particular component \cite{bird2011don}. On a similar note, Kelly writes that ``the scientist is an integral part of the software system and cannot be excluded from its consideration'' \cite{kelly2015scientific}. Looking via interviews at the perspective of computer scientists engaged at middle-ware development, Lee et al \cite{lee2012sociotechnical} ``examines how technologists develop and sustain middle-ware applications over time by leveraging expertise and partnering with different research domains in order to achieve log-term infrastructural goals. Activities to ``sustain their software development research agenda well beyond the lifetime of project  Lee et al have also emphasized the intentional personal networks as key - the human infrastructures that sustain cyber-infrastructure projects \cite{lee2006human}.

There are different ways to bound scientific software, relative to other kinds of software work. The place of software has been broadly categorized as ``including data analysis, simulations, and managing workflows''  \cite{howison2011scientific}, and as deeply integrated with scientific pursuit in \cite{sletholt2011literature}. Kelly bounds her study of scientific software production as extending to ``application software that includes a large component of knowledge from the scientific application domain and is used to increase the knowledge of science for the purpose of solving real-world problems,'' and excludes ``software written to become a commodity product ... to verify [safety procedures] [and] to control equipment'' which ``has to be correct, to the exclusion of all else'' \cite{kelly2015scientific}. In addition to coding per se, we may also include end-user programming using complex systems that present the scientific community with an astounding array of possibilities, all of which demand skill-acquisition and attention. LabVIEW (see \cite{whitley2001visual}) and Excel are commonly-used ``tools'' but also ``end-user programming'' environments, with barriers to learning not unlike those of applying programming skills, such as design and difficulty in understanding and explaining unexpected behavior \cite{ko2004six}. Tools can also be intended to support learning: Cervantes et al have considered ways in which skill-based engineering expertise may be shared in the eScience context \cite{cervantesnew}, such as via wikis. %The software engineering studies of scientific software work reviewed in this chapter (e.g., \cite{heaton2015claims}) distinguish scientific software production as unique from other (industry) coding contexts. Meanwhile,

Kelly points out that scientists identify their coding practices largely by non-adherence to a particular software production methodology \cite{kelly2015scientific}. This ``amethodical'' narrative is presented in contrast to the methodical one by Truex et al \cite{truex2000amethodical}. The methodical, privileged approach to code work holds that software systems development is more controlled and rational than it is in lived practice of industry software engineering. Truex et al create an alternate narrative that ``occurs in completely unique and idiosyncratic forms''. In the lived reality of coding, neither exists in its elementary state, but the amethodical narrative is marginalized in reflection about software engineering practices. Building on this work, Philip et al study amethodical remixing, or the use of copy and paste and snippets \cite{philip2012software}. Though these practices are not included in textbook, privileged accounts of development, they are nevertheless present. Implications offer design of tools that support more realistic everyday scenarios of re-mixing, which blend the dialectic methodical and amethodical narratives.

%

%He continues: as in manufacturing and medicine, investments in quality repay themselves several times over because mistakes are more expensive to fix than to prevent.'' In Kelly's articulation of the subject of herrisk-averse programmer'' she distinguishes this code - which MUST work - from the experimental kind which is undertaken by many scientists [kelly]


\subsection{Best Practices: The Universal Necessity of Sharing}

In this section, I pull together discussions on programming best practices, as well as meta-discussion on the matters of the practical and the beautiful in code work. In synthesizing both of these bodies of work, I distinguish code products from code practice. Additionally, I focus on sharing and making shareable as the two main values.

The term ``best practices'' refers to industry-wide, shared ideas about the best way to do things. These  often stress putting code (and data) online for others to access \cite{stodden2013best}, even if it cannot run but at least as a documentation of scientific activity \cite{mislan2016elevating}. Wilson et al offer 8 concrete directions, including things like ``plan for mistakes (turn bugs into test cases)'' which connect a minor expected roadblock or problem (a bug) into the opportunity for a ``best practice'' act (testing) \cite{wilson2014best}. The suggested practices from Baxter et al are a bit more high-level: ``1) design the project up-front; 2) document programs and key processes; 3) apply quality control; 4) use data standards where possible; and 5) incorporate project management'' \cite{baxter2006scientific}.

In addition to the software-related strategies and practices above, additional practices concern making data more effectively available through ``deliberate documentation'' and persistent quality access \cite{baker2008enabling}. The list of best practices is in constant flux, as noted by Sletholt et al who conduct a review of agile practices reported in scientific publications, and who characterize scientific practice as distinct from other contexts in that it is relatively more ad-hoc and grounded in experience \cite{sletholt2011literature}.


In his reflection on the design and implementation of the Software Carpentry curriculum for well over a decade, Wilson writes that computational scientists do not have the publishing incentives to prioritize code quality, so the course does not attempt to ```sell' quality directly... Instead, it starts from the fact that the only way to improve productivity is to improve quality'' \cite{wilson2013software}. The claim of the ``fact'' is an illustration of the relationship between moral and material aspects of code work:

\litquote{leach2009freedom}{A central component of the F/LOSS  ethos is that working openly and sharing the source code of software enables improvements to evolve more effectively, and that as a whole ``better'' software is produced. The concept of `better software' (a material judgement), which arises from `better' processes of production (a moral judgement) conceals another complex series of understandings and judgements generated by familiarity with and proximity to the workings of the machine (computer) itself ... the beauty of code comes to be an aim in itself in this overlap of practical and moral truth.}

Leach reports on an ethnographic study of free/libre and open-source software (F/LOSS), going on to point out how his informants would often claims that the ``messiness'' of proprietary software is the primary reason it remains inaccessible, and that its producers ``would dare not to open source their code because [of shame over] how its functionality was achieved.'' The informants quoted in my dissertation often expressed hesitance to open up source code on account of its messiness, which is distinct from the concern over attribution as articulated by Enke \cite{enke2012user}. ``I am not a real programmer'' and ``it works, but it's not the most elegant way to do things'' were common asides among the participants of this study when describing their own work. The craftsmanship of code, as elevated by the F/LOSS ethos, trickles into the self-reflective scientific code discourse and touches on ``open science'' and related notions. For example, Software Carpentry has engaged in the F/LOSS community and value system since its inception in the 90s \cite{wilson2013software}. The interpretations of ``best practices'' for scientific coding invariably stress sharing in general and sometimes open source in particular \cite{mislan2016elevating,stodden2013best}. 

The mechanics are distinct between these notions, but they all embody a common value of sharing, which is simultaneously powerful enough to warrant a great deal of discussion and non-specific enough for that discussion to mostly focus on critiquing shortcomings. Howison and Herbsleb identify several different \textit{software production systems} articulated in terms of the social and institutional contexts and in incentives \cite{howison2011scientific}. One these software production systems is \textit{parallel software practice}, which is similar to what Paine and Lee focus on in \cite{paine2014producing}, and which Howison and Herbsleb do not critique. On the other hand, the authors suggest that a ``dual science and software practice is a somewhat questionable proposition for working scientists'' because aside from creation effort, the maintenance effort is unreasonable and inescapable. This maintenance effort is the ``extra work'' that Trainer et al identify as motivate primarily by social means \cite{trainer2015personal}. For example, direct interpersonal exchange over email or with a past colleague can help prioritise an endless list of possible features with unclear ramifications for scientific users. The discussion of incentives and motivations concerns itself with the \textit{material}, while the discussion of increasing sharing or code ``messiness'' concerns itself with the \textit{moral}. Leach notes that the material goodness of better software is achieved by the moral goodness of better practice. The assumption behind critiquing and problematizing incentive structures reverses the relationship, making the moral goodness of open-science-related concepts accessible through the material goodness of rewarding effective code work practices.



\subsection{Coping with Code}

Vocational shifts  in the amount of computational work affect different areas and fields differently. The coding practice of a computational modeling researcher can come under critique for lacking abstraction or formal testing \cite{heaton2015claims}. The researcher who works with increasingly large amount of field data that demand learning new ways of automation is urged not only to learn and use these even-evolving skills, but also share the results as an increasingly important record of scientific work \cite{mislan2016elevating}. The broadly-scoped values and philosophies of sharing, automation, and scalability are in practice implemented by way of ``small-scale and immediately practical issues,'' which is why one of the most popular workshops for scientists learning programming calls itself Software \textit{Carpentry} rather than Software \textit{Engineering} \cite{wilson2006software}. Section 2.1 reviews literature that investigates, critiques, or seeks to change \textit{software practice in scientific contexts}. Section 2.2 subsequently expands to include \textit{designed interventions} that seek to transform scientific practice, spanning both tools (e.g., visual analytics pipelines) and social initiatives (e.g., Software Carpentry workshops).

Technological capabilities change over time, particularly in observational oceanography, and those affected undertake a variety of strategies to cope with change, including learning  \cite{steinhardt2015anticipation}. On the basis of an ethnographic study of a large oceanography data infrastructure collaboration, Steinhardt and Jackson introduce the notions of ``anticipation work,'' which includes ``the mundane, local, and sometimes highly personal accommodations to the future that always accompany the more formalized and/or speculative dimensions.'' One of the examples of these ``highly personal accommodations to the future'' was the intention to learn new skills in anticipation of the next career move after the infrastructure project had wrapped up. With time as an analytic lens in an ethnographic study, Chen et al point out that the demands on human time to learn the necessary skills and to use them to make a multi-component system behave as desired are a notable conflict in the lifecycle of a scientist's use of a high-performance computing (HPC) system \cite{chen2015considering}. Nevertheless an overwhelming majority of programming work done by scientists is not with large systems, as Hannay et al find \cite{hannay2009scientists}.  Regarding databases, Franklin et al argue for co-existence over integration, where ``semantic integration evolves over time and only where needed'' because ``the most scarce resource available for semantic integration is human attention'' \cite{franklin2005databases}; though the existence of integrated databases would present an improvement over the inefficient manual data-retrieval and transformation currently in place among many scientists \cite{enke2012user}. 

In a recent summary of the adaptations necessary ``in the face of changing science'' \cite{bietz2012adapting}, Bietz and Lee highlight the tension of creating contextualized, tailored software solutions against the maintenance and design of a long-lived cyber-infrastructure. In a similar fashion that foregrounds software construction and data use in the study of scientific collaborations, Ribes and Finholt identify as a major tension in cyber-infrastructure development between ``respecting current work practices'' and ``transforming scientific practice'' \cite{ribes2009long}. Endeavors defined by their ambition to transform scientific practice into a more collaborative endeavor able to make use of orders of magnitude more data are therefore not only risky technological undertakings, but are also associated with tensions across values and meanings between communities involved. In considering the failed as well as the successful infrastructures within the eScience umbrella, Zimmerman points out a gap between ``how systems work and how users expect them to work,'' and, based on interviews of ecologists, notes that their ``collection of data for re-use mirrors the standards that guide the gathering of their own data in the field or laboratory'' \cite{zimmerman2007not}. 

\subsection{Visual Meaning-Making}

Gilbert makes the case for visualization as a crucial metacognitive skill in science and, inextricably, science education \cite{gilbert2005visualization}. Grochow et al introduce a hybrid client-and-cloud analysis and visualization environment to support the many kinds of comparisons and transformations necessary \cite{grochow2010client}. Their system is based on Trident, and includes improvements in efficiency \cite{barga2008trident}. ROMSTOOLS also offers an ``integrated toolbox'' with some global datasets (such as SeaWIFS, a global satellite image which uses color to estimate surface biomass over the recent decades) and MATLAB programs for the ROMS ocean simulations, noting that ``tools for visualization, animations and diagnostics are also provided'' \cite{penven2008software}. Other tools include visualization environments (e.g., ocean data view \cite{schlitzer2002interactive}, Trident \cite{barga2008trident}, COVE \cite{grochow2009cove}, and COOS \cite{li2014visualization}), data management approaches for the specific challenges of data collection at sea \cite{bechini2013management}, programming environments to support the writing of better software (e.g., \cite{palyart2011improving}). Interventions aimed to augment or transform various different parts of the existing scientific process much contend with an even broader interpretation of ``tool,'' for example noting that ``the success of paper field notebooks can be attributed to their resiliency to damage in rugged terrain -- a sheet of paper torn in half becomes two; a tablet computer torn in half does not beget two computers'' \cite{yeh2004field}. These as much as the products of interactive visualizations constitute a record of transformation in the course of scientific work \cite{latour2013laboratory}. In an ethnographic work of the team studying Mars via the Mars \textit{Rover} and the images it produces, Vertesi articulates ``\textit{drawing as}'' as an ``analytical frame'' with which ``instead of asking what is drawn, asking how and what it is \textit{drawn as},'' and which, thereby:

\litquote{vertesi2009seeing}{....requires the analyst to inquire into the work involved in crafting an image that can be taken up in practice as a transparent representation of the object in question. This bypasses questions of reference -- how the image is tied to an object in the external world or whether the depiction reveals the object's essential nature -- to reveal how image-making in science inscribes a scientific community's values, organization of work, or epistemology onto the object at hand}

\subsection{Summary}

Software also plays a role in collecting, handling, analyzing, displaying, and sharing observational or field data. As an example of software as a component of a more larger data collection scheme, consider the following abstract excerpted from a 1996 publication by researchers from Woods Hole:

\litquote{davis1996rapid}{Traditional methods for determining spatial distributions of planktonic taxa involve net, pump, and bottle collections followed by the tedious and time-consuming task of plankton sample analysis. Thus, plankton ecologists often require months or even years to process samples from a single study. In this paper, we present a method that allows rapid visualization of the distribution of planktonic taxa while at sea. ... We describe the techniques used in imaging the plankton, analyzing the video, and visualizing the data. We present an example of at-sea data analysis conducted aboard [research vessel at specific place on a specific month] and visualizations of the 3-dimensional distribution of selected planktonic taxa in a 2 2 km 90 m volume of seawater. A video of the image processing and visualization is included on the CD-ROM accompanying this volume and is an essential part of this paper.}

This paper has, early on, a section on ``system requirements'' for the ``fully automated at-sea analysis of plankton size and taxonomic composition'' has three sections: ``(1) High-quality images of individual plankton ... (2) An image processing and pattern recognition system... [including] (a) a software/hardware system ... for preprocessing of the images (including realtime image capture, object detection, focus detection, and digital storage of the region-of-interest); (b) pattern recognition algorithms ... for automated identification and classification of planktonic taxa; (c) the pattern recognition algorithms must be transferred to high-performance image analysis hardware in order to achieve real-time processing capability; (3) a dam processing and visualization system must be developed to analyze and display the distributional data at sea in real-time'' \cite{davis1996rapid}.

The requirements are all centered around enabling \textbf{real-time} analysis that is \textbf{not otherwise possible}. Through the rest of the paper, they describe an implementation that actually includes a human intervention component standing in for something that would, in a possible future, be automated. The real-time processing is enabled by any means necessary: a trained researcher on board, specialized hardware for better performance, or whatever is both feasible and enables operation at the viable pace. The paper describes an ``interim solution:'' a ```point-and-click' user interface for analyzing [regions of interest] obtained at sea and from archived video tapes.'' the authors also go into detail regarding the ``underwater video system,'' in terms of its specifications and construction, references to a prior publication, and the contexts in which it has been deployed. %\kt{context for this paper- why chosen}

I chose this paper as an anchor because it both describes a tightly-coupled software-hardware-user system, but also because it provides more concrete context to the discussion of code in science, and code in oceanography in particular. The following section delves into this more deeply.