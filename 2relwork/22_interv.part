\section{Interventions}

Technological capabilities change over time, particularly in observational oceanography, and those affected undertake a variety of strategies to cope with change, including learning  \cite{steinhardt2015anticipation}. With time as an analytic lens in an ethnographyc study, Chen et al. point out that the demands on human time to learn the necessary skills and to use them to make a multi-component system behave as desired are a notable conflict in the lifecycle of a scientist's use of a high-performance computing (HPC) system \cite{chen2015considering}. Nevertheless an overwhelming majority of programming work done by scientists is not with large systems, as Hannay et al. find \cite{hannay2009scientists}.  Regarding databases, Franklin et al argue for co-existence over integration, where ``semantic integration evolves over time and only where needed'' because ``the most scarce resource available for semantic integration is human attention'' \cite{franklin2005databases}; though the existence of integrated databases would present an improvement over the inefficient manual data-retrieval and transformation currently in place among many scientists \cite{enke2012user}.

This section includes programming education interventions, as well as software tools, and  infrastructural efforts that are designed to complement, supplement, or transform scientific practice, because they all share the common property of presenting the scientific community with an astounding array of possibilities, all of which demand skill-acquisition and attention. LabVIEW (see \cite{whitley2001visual}) and Excel are commonly-used ``tools'' but also ``end-user programming'' environments, with barriers to learning not unlike those of applying programming skills, such as design and difficulty in understanding and explaining unexpected behavior \cite{ko2004six}. Tools can also be intended to support learning: Cervantes et al. have considered ways in which skill-based engineering expertise may be shared in the eScience context \cite{cervantesnew}, such as via wikis.

The place of software has been described and categorized as ``including data analysis, simulations, and managing workflows'' by Howison et al \cite{howison2011scientific} \tk

\kt{Ways to classify forstware} - via Howison: There are two examples of work towards this end. Sanderset al. provide a survey of scientific software that specifi- cally considers the organization of its production, such as whether it was produced by commercial interests or open source projects, albeit as one source of correctness risk [14]. Segal and colleagues approach scientific computing through the lens of end-user software development, a branch of SE research that recognizes that the practice of software development regularly occurs outside the traditional role of professional software developers [16, 17, 18]. They identify two situations: “when the software is intended for use either by the developer herself/himself or by closely co- located colleagues, for example, people working in the same laboratory” which they contrast with software developed “within a closely co-located group but intended for the wider scientific community of which the developers form a part” [17]. .. While these distinctions in practice and intended use are important we believe that they can be improved through a focus on the incentives faced by scientists producing software, \tk

\tk BLAST and sequencing components via Howison?? "BLAST Webservice The final three steps compare the sequences with known sequences by comparing against public databases. This process involves an algorithm known as BLAST, in the words of our informant, ``BLAST is the most important and most useful and most used piece of software in biology.” This algorithm finds appropriate matches in the strings of DNA sequences." "Private “power-user” scripts Our Structural Biology in- formant described himself as a power-user meaning he preferred speed and accuracy (command-line interface, or cli) over ease of use (graphical user interface, or GUI)." Our informant held it as axiomatic that a paper, in the methods and materials section, should provide sufficient detail for its replication. However, he considered it only a limited responsibility to assist anyone attempting such a replication; they must “do their homework.” He did not consider it likely or appropriate for readers, or even reviewers, to seek access to his personal scripts. He considered these scripts to be quite different from other types of software. If he had written a novel tool embodying a non-intuitive technique and used it in a paper, he would feel responsible for releasing that.

Aside from the oceanography teams, one of my fieldwork sites were Software Carpentry (SWC) workshops. The workshops have been volunteer-taught and run since 1998 with the goal of ``teaching researchers in science, engineering, medicine, and related disciplines the computing skills they need to get more done in less time and with less pain,''\footnote{via http://software-carpentry.org/about/}. Its increasingly popular curriculum  can be taught by volunteer instructors who attend a training. The curriculum spans automation (with shell), collaboration and version control (with GitHub),  database management (e.g., SQL) and programming. The programming lessons require the shell and version control prerequisites, and are taught in R in RStudio or Python in iPythonNotebook. In those workshops I observed, either only Python was taught (see Section 7.2), or the two options were offered in in concurrent sessions serving distinct audiences, and including data representation all the way to visualization.

Gilbert makes the case for visualization as a crucial metacognitive skill in science and, inextricably, science education \cite{gilbert2005visualization}. Grochow et. al. introduce a hybrid client-and-cloud analysis and visualization environment to support the many kinds of comparisons and transformations necessary \cite{grochow2010client}. \kt{client and cloud based on trident system; thisis about implovements in efficiency and stuff} ROMSTOOLS also offers an ``integrated toolbox'' with some global datasets (such as SeaWIFS, a global satellite image which uses color to estimate surface biomass over the recent decades) and MATLAB programs for the ROMS ocean simulations, noting that ``tools for visualization, animations and diagnostics are also provided'' \cite{penven2008software}. Other tools include visualization environments (e.g., ocean data view \cite{schlitzer2002interactive}, Trident \cite{barga2008trident}, COVE \cite{grochow2009cove}, and COOS \cite{li2014visualization}), data management approaches for the specific challenges of data collection at sea \cite{bechini2013management}, programming environments to support the writing of better software (e.g., \cite{palyart2011improving}). Interventions aimed to augment or transform various different parts of the existing scientific process much contend with an even broader interpretation of ``tool,'' for example noting that ``the success of paper field notebooks can be attributed to their resiliency to damage in rugged terrain -- a sheet of paper torn in half becomes two; a tablet computer torn in half does not beget two computers'' \cite{yeh2004field}. These as much as the products of interactive visualizations constitute a record of transformation in the course of scientific work \cite{latour2013laboratory}. In an ethnographic work of the team studying Mars via the Mars \textit{Rover} and the images it produces, Vertesi articulates ``\textit{drawing as}'' as an ``analytical frame'' with which ``instead of asking what is drawn, asking how and what it is \textit{drawn as},'' and which, thereby:

\litquote{vertesi2009seeing}{requires the analyst to inquire into the work involved in crafting an image that can be taken up in practice as a transparent representation of the object in question. This bypasses questions of reference -- how the image is tied to an object in the external world or whether the depiction reveals the object's essential nature -- to reveal how image-making in science inscribes a scientific community’s values, organization of work, or epistemology onto the object at hand}

In a recent summary of the adaptations necessary ``in the face of changing science'' \cite{bietz2012adapting}, Bietz and Lee highlight the tension of creating contextualized, tailored software solutions against the maintenance and design of a long-lived cyber-infrastructure. In a similar fashion that foregrounds software construction and data use in the study of scientific collaborations, Ribes and Finholt identify as a major tension in cyber-infrastructure development between ``respecting current work practices'' and ``transforming scientific practice'' \cite{ribes2009long}. In considering the failed as well as the successful infrastructures within the eScience umbrella - defined by their ambition to transform scientific practice into a more collaborative endeavor able to make use of orders of magnitude more data - Zimmerman points out a gap between ``how systems work and how users expect them to work,'' and, based on interviews of ecologists, notes that their ``collection of data for re-use mirrors the standards that guide the gathering of their own data in the field or laboratory'' \cite{zimmerman2007not}.

Baker et al. introduce \emph{Ocean Informatics}, an area that inhabits intersection of oceanography, information sciences, and social sciences, drawing on the lessons learned from two independent programs with some common data management practices from over a decade \cite{baker2008enabling}. Below, I summarize the most salient points of the timelines of two programs from Baker et al.'s manuscript: Palmer Station and JGOFS.

In 1990, the program Palmer Station was established as a long-term ecological research (LTER) site, and the data was made available online in the form of static text. Given that the history of triangulating data and theory reaches back almost a century (\cite{kunzig2000mapping}) and the history of computational modeling over half a century (\cite{edwards2010vast}), the formalization of data access through an online endpoint is relatively recent. Nevertheless, it still predates Apache Subversion (founded 2000) and GitHub (founded 2008); it also predates the initial work on the iPython Notebook \cite{perez2012ipython}, which began in 2001 although the stable release came over a decade later. Aside from the point of precedent, I also want to highlight that because of the need for comparisons over time spans, time scales, and between geographical regions, long-term research sites are especially useful. This also means that data is available now about the era following the 1990s at scale and resolution that did not exist before that time, or exists in inaccessible format. The stewardship of \textit{historical} data is also important, not just forward-looking data infrastructures. \kt{TODO want citation.}

Over the next decade, the capabilities of JGOFS illustrate that a data endpoint can be arbitrarily complex and feature-rich:

\litquote{baker2008enabling}{From its beginning in 1994, JGOFS had a ``Data Management Office'' with technical staff that worked together with investigators; ``much of the collaboration focused on issues related to quality control and the collection and subsequent publication of complete metadata for contributed data sets.'' ``All process study data were ingested into an object-oriented, relational database and made available via the World Wide Web. Using a standard Web browser client, users of the JGOFS data system can generate custom data sets that match their research interests by combining multiple data sources `on-the-fly'.'' In early 2000s, ``to meet requests for data queriability and requirements for networking,'' new system was designed featuring ``online data access, strategic integration and visualization.'' ``Data and metadata management is offered through web interfaces with tiered permissions that enable data provider participation in making their data accessible. The new system is built upon a relational database with an object-oriented API layer that supports Web-based data query.''}

The comparison of findings and predictions includes model output, which is addressed by the solution  in \cite{howe2008end} and ocean data view \cite{schlitzer2002interactive}, both of which focus on readjusting the grid and thereby enabling coherence between disparate data sources \kt{WRONG CITATION HERE-  ITS THE OTHER ONE!!!}. Enke et al., writing about barriers to biodiversity data sharing, also note the importance of combining observational and model data in data access endpoints \cite{enke2012user}. Baker et al. write: ``As the JGOFS program transitioned from process-oriented field studies to modeling, the data system was extended... Synthesis and model results, larger in volume and often global [rather than regional] in scope... required a more graphically oriented user interface and extended visualization capabilities'' and the technical staff ``worked closely with investigators to provide timely availability of data during the active research phase and to ensure preservation of the completed data collection as an important part of the [program] legacy'' \cite{baker2008enabling}.

\ca{want more background about the programs, is JGOFS abotu biodiversity}
\ca{unclear transition}

There are many reasons for reluctance in data sharing for biodiversity data, as Enke et al. report on the basis of  over 60 interviews and  over 700 survey responses \cite{enke2012user}. Despite ``clear commitment to share biodiversity data,'' the authors find ``reluctance to actually do so due to a mixture of social and technical impediments, such as loss of control over data and lack of professional reward for sharing.'' In addition to reluctance to share data, especially data not associated with a publication, time and energy constraints constituted a challenge: ``Many researchers already feel they have too many responsibilities. The average total time researchers spend with data curation is more than a week per year.'' \ca{doesn't seem like too much} In a study of land-change science, another interdisciplinary field where data sharing is both vital and not standardized, Young et al. suggest the strategy of ``participatory meta-analysis'' where ``case study authors are enlisted to either code their cases based on a pre-defined classification scheme or to confirm whether or not their cases have been coded properly. As incentive for their participation, they are offered co-authorship on the resulting meta-analysis;'' in other words, data authors are provided scaffolding to make adding necessary data descriptions easier, as well as rewarded professionally \cite{young2015re}.
%"  "To increasthe amount of shared data curation, management and sharing of data has to be better integrated into the researcher's everyday work flow. A first step is that data should be collected already in a database compatible format (databases could provide e.g. exemplary Excel sheets that could then be uploaded directly). This would save time otherwise spent on editing and converting data into the right format." ".""The participants of our study also felt that the quality of published studies could increase through the transparency of the underlying data sets."""In the interviews most participants wanted to extend their own data sets with similar data to put their research into a broader context. " "The quality of data for reuse is often judged by the author of the data set. If researchers know the submitter of the data set and rate her or his work as sound they are more likely to use their data""Additional information should include details on how, by whom, when, where and why data were collected"

\kt{literally NO ONE gets this transition}
\kt{from talking with fogarty, make this chapter follow more tha talk outline?}

Borgman describes data as the ``glue'' of collaboration between scientists and technologists \cite{borgman2012whos}. Indeed, open data is a crucial means to integrate and accumulate knowledge \cite{reichman2011challenges} in a multidisciplinary environment, particularly one that is ``radically unstandardized'' \cite{steinhardt2015anticipation}. From the software engineering research side, particularly when it comes to numerical modelling traditions in the sciences, there is a ``lack of separation of concerns,''\footnote{Patel et al., in a study of programmers attempting to implement machine learning applications and having difficulty because they focus too much on algorithm selection and neglect data debugging \cite{patel2008examining}. This can be summarized as the opposite of the indictment that the scientists worry too much about their data and not enough about their code, leaving us with the unremarkable hypothesis that in problem-solving, people focus on what they know.} no sense of abstraction, where ``the problem to be solved entirely [is] mixed with numerical approaches and target-dependent information'' \cite{palyart2011improving}. Heaton and Carver note a similar claim in their systematic review of claims about software engineering practices in the sciences, specifically modeling \cite{heaton2015claims}. Sletholt, Hannay et al. also find that ``the definition of test cases for validation and verification of the software is  perceived as challenging ... it is often not obvious to  stipulate whether an error lies within the scientific theory or in the  implementation (numerical approximation) of that very theory'' \cite{sletholt2011literature}. In this sense, the software can also be described as ``glue'' - once applied, inextricably and irreversibly linked to the other components. Or, as Ragan et al. note in a Nature article on programming environments for microbial ecology: ``Although computation can be a language that bridges many disciplines, additional `glue' is often needed to make the requirements mutually comprehensible to diverse members of a project team'' \cite{ragan2013collaborative}. Here, the ``glue'' is more the kind of ``social glue'' that Henderson identifies in her work on engineering sketches,  where physical, flexible representations which are preferable the more rigid technological environments because they act - by virtue of their flexibility - as a ``social glue'' between different communities of practice \cite{henderson1991flexible}.

The ``glue'' that holds this chapter together, in its wide-ranging review of half a dozen fields of scholarship, is the existence of scientists who take part in - or want to take part in - many different kinds of code work in the course of scientific craft. The study of adoption of tools is then linked to programming skill acquisition, which is in turn influenced by the context of the scientist and the extent to which certain kinds of code work are pervasive or rewarded. In the study of large collaborations (e.g., oceanography \cite{steinhardt2015anticipation} and radio astronomy \cite{paine2014producing}) or other contexts where there is relatively clear separation of skills and interests (e.g., data gathering at sea with robots \cite{borgman2012whos}), communication and collaboration practices are the focus of study. Outside of these massive projects, the principal investigators (PIs) are responsible for managing diverse groups with disparate skill-sets and backgrounds, for determining which innovations to invest (time, money, and energy) in, for creating additional roles within the group, and for advising the early-career members of that group not only on success within it but also beyond. This may be especially salient in disciplines like oceanography, where there are many different research methods and many different levels of associated collaborative work or major expenses, including software licenses, laboratory equipment, staff roles.