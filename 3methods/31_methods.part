\section{Methods}

All data were collected using a combination of interviews and observations. The observations focused on either (1) events and meetings, such as regular lab or group meetings and special programming skills workshops, or (2) the mundane daily activities of particular researchers, whom I was ``shadowing'' that day. Though up to a hundred scientists had cumulatively been observed over the course of the various events of interest, the core of the study is comprised of 21 oceanographers across the groups who were actively adopting/adapting new programming skills over the course of at least a year. An \textit{additional} 25 oceanographers were in the same groups and interacted frequently with the core 21 in projects I included in the study, and which are described in the following chapters. Seven out of the total 46 oceanographers were also interviewed about, and initially recruited through, the Software Carpentry (SWC) workshops, in addition to 13 scientists who are not oceanographers. The study participants, including its dual inclusion criterion and overlapping recruited populations, is summarized in Figure \ref{fig:pops}.

\begin{figure}[tbh]
    \centering
    \includegraphics{3methods/Pops.pdf}
    \caption[Study participation summary, grouped by inclusion criterion.]{\textbf{Study participation summary, grouped by inclusion criterion.} This figure illustrates the overlap between participant pools of scientists interviewed after attending SWC-like events, and oceanographers recruited via SWC and Data Science Environment at UW.}
    \label{fig:pops}
\end{figure} 

Data collection was conducted over the course of 18 months, from April 2014 to December 2015, aiming to get a breadth of examples of software adoption and adaptation. There were, roughly, three stages: first, I recruited and interviewed SWC participants. Second, I recruited the four teams that comprised the core population, and did initial fieldwork by  attending regular meetings and events, such as weekly lab meetings, regular joint meetings with computer science collaborators, and so on. Third, once I had a better idea of the various projects in each team and a sense of the group dynamics, I conducted the bulk of my observations by ``shadowing'' each of the core participants anywhere between several hours and multiple days. Because my time was split between the various teams, I conduced ``check-ins,'' or very informal interviews initiated over email, or casual half-hour chats in a hallway, over brown-bag lunch, or over coffee. 

Only the first set of the SWC interviews, as well as the reflections of the team which did the least code work, were audio-recorded. The audio recordings were transcribed and added to the dataset. All other interviews and observations were in the format of handwritten field-notes which I then typed up. Occasionally, these field-notes included excerpts from various emails, such as a ``meeting summary'' sent out of a joint meeting, or an interestingly-worded event reminder that helped to understand the context and significance of that event. The data is around 300K words\footnote{All of which are stored in private git repository in plain text with a metadata header and verbose filename. No qualitative analysis software was used. I had written a few simple python scripts to process the data, but ultimately relied mostly on grep and memo-writing for all analysis tasks. Some images were stored in a folder in the repository and there were not so many of them, and they were not so big, as to warrant any specialized process.} in total, though the density of words varied as my initially verbose style of note-taking became more condensed and focused over time.

The original recruitment from the first stage was focused on Software Carpentry participants; 20 participants were each interviewed between 1 and 3 times over the course of several months, depending on the extent to which code work and new programming skills were a part of their daily activities following the event. Of those 20, 7 were oceanographers from 2 different groups that were then included in the study. During the second stage, I reached out to both groups and included them in the case study of oceanography. The remaining 2 of 4 total oceanography groups were included based on their engagement with the Moore-Sloan Data Science initiative on the UW campus. In this way, the \textbf{inclusion criterion} required all study participants to have actively engaged with a particular zeitgeist of increasing/improving coding skills in scientific work; all had spoken of ``best practices,'' though mostly in a context of \textit{striving} rather than \textit{currently doing}.  By ``engagement,'' here, I mean not unanimous embrace, but a collective sense of ``this is the way things are going,'' as one SWC interviewee commented in explaining her decision to spend time learning Python despite not having a clear, immediate use for it. In other words, not all participants were unequivocally enthusiastic about the increasing amount of programming competence expected or required for professional success; many had reservations.

\subsection{Group Events}

Workshops, ``skillshares,'' ``sprints,'' ``joint meetings,'' and other such events are special in that they allow their interdisciplinary scientific participants to explore and enact their collective imagination of what is possible in the field. The idea of the collective imagination will be developed further  Chapter 4. Figure \ref{fig:events} shows the different events, distinguished along the degree of closeness of collaborative work, and how targeted that work is. Here, ``work'' refers to ``code work'' but overlaps with the practice of science more broadly (in terms of idea exchange, brainstorming, feedback, and critique) because code is not readily separable from its scientific context (as noted in Section 2.1; see esp. \cite{kelly2015scientific,paine2014producing}). Table \ref{tab:counts} shows the amount of observations and interviews in the data set as a whole, including both oceanographer and SWC-specific populations.

\ca{below, in Fig 3.2 or the events fig,, call it interactivity instead of closeness}

\begin{figure}[tbhp]
    \centering
    \includegraphics{3methods/events.pdf}
    \caption[Summary of the different \textit{group} events included in they study, distinguished by closeness and targeted-ness of work.]{\textbf{Summary of the different events included in the study, distinguished by closeness and targeted-ness of work.} \textit{Closeness} refers to the extent to which multiple people are actively working together on the same piece of code, and the extent to which the work is \textit{targeted} refers to how obviously-articulated (or pre-meditated) the aim of the event is. Excluded from this chart is ``shadowing'' which focuses on the individual, though may involve also participating in some of these events.}
    \label{fig:events}
\end{figure}


\begin{table}[htbp]
    \centering
    \begin{tabular}{r|c|c|c|c|c}
        Type & BGC-M & CI-L & O-L & RN-M & SWC/etc \\
        Joint Meeting & 1.5 (3) & 8.5 (8) & 13.37 (9) & 1 (1) & N/A \\
        Demo (+) & 1.5 (1) & 1.5 (2) & 2 (2) & 2.5 (2) & N/A \\
        Feedback (+) & 2.5 (2) & 3 (4) & 2.5 (2) & 6 (5) & N/A \\
        Talk (inc. practice) & 6.12 (4) & 2.25 (2) & 2 (2) & 5 (3) & N/A \\
        Talk (practice only) & 4.65 (3) & 1.5 (1) & 1 (1) & 1 (1) & N/A \\
        Shadow & 57.08 (8) & 28.25 (4) & 4 (1) & 36.75 (2) & N/A \\
        Casual (+) & 4.95 (7) & 4.5 (5) & 0.5 (2) & 0.5 (1) & N/A \\
        Interview & 0 & N=3 & N=5 & N=3 & N=17 \\
        Workshop & 3.5 (2) & 3.75 (2) & 0 & 19.5 (4) & 9.25 (3) \\
    \end{tabular}
    \caption[Qualitative data set summary, with hour durations.]{\textbf{Qualitative data set summary. }A breakdown of data collection across the four groups and over all the different event types, which totalled just over 300 hours, and excludes the interviews and observations conducted by my colleagues that were pertinent to the study and included in the analysis. All amounts are listed in the format ``hours (instances)'' - so, for example, ``2.5 (2)'' means that a total of 2.5 hours, over 2 different occasions, was observed of that event type (row) for the group (column). The interviews are listed by count rather than time measurement; they lasted between 30 minutes and 3 hours, and the cumulative total time is not meaningful to compare across the columns. ``+'' means that this was a pervasive dynamic throughout many casual interactions and therefore the given number is a conservative amount. Across the top, I included short-hand labels for the 4 oceanography groups in the study, introduced in Section 3.3 as BioGeoChem-Model, CustomInstrument-Lab, Omics-Lab, and RegionalNowcast-Model groups; as well as the participants interviewed about their experiences with SWC-like events. Note that the ``Interviews'' do not include ongoing more casual check-ins. Of the 20 interviews for SWC, 7 are listed under O-L and RN-M, reflecting the overlap in participant populations shown in Figure \ref{fig:pops}. The interviews in group CI-L refer to more structured interviews with team members whose role had changed, on the subject of that role change.}
    \label{tab:counts}
\end{table}


By distinguishing \textit{closeness} of work, I try to highlight the range of whether multiple people are closely engaged with the same piece of code at the same time. For example, being around people to be encouraged, share momentum, or be exposed to ongoing scientific work for general awareness is on the ``low closeness'' end of this axis. In this case, the code written can be an extension of a single individual's thought process, and follow whatever standards that person holds their code to. The ``medium'' case includes, for example, asking for help or feedback when needed, where the code written by an individual \textit{does} become part of communication, feedback, or negotiation (reaching out for ``high-level feedback'' involving no code would be ``low,'' not ``medium'' on the ``closeness'' axis). The ``high'' end of this qualitative spectrum involves working closely with at least one other person who also has a stake in that particular work, which requires all those involved to negotiate (or have, in the past, negotiated) a consensus across the differences in their working practices.

The extent to which this work is \emph{targeted} refers to how much the work is aimed at a particular goal or project. For example, a group sprint by one of the teams\footnote{The RegionalNowcast-Model team, which is introduced in Section 3.3} spanned a variety of possible tasks and had room for suggesting other additional undertaking, but all the projects were in service of a stakeholder-facing web visualization dashboard that existed in addition to individual researchers' scientific agendas\footnote{This was a major data stewardship deliverable of the group, but was not itself the aim or domain of any particular researcher}. I distinguish this from the ``Joint Meeting,'' or ``Feeling out [a Possible] Collaboration,'' which are meetings 1-on-1 between PIs or more senior researchers, or between multiple members of different groups, with the aim of considering possible overlaps and mutual motivations. This is less targeted than working on a joint artifact as a group, but more targeted than practice talks or dissecting a particularly interesting paper, which can exist without directly contributing to an active line of research. For example,seeing a guest lecture, for example, may not have any immediate application to a specific ongoing task, or be only a open issue; however, it is an essential part of the overall practice of doing science. Demos and workshops for ``tangential'' tools of skills may be an immediate distraction from a task at hand, while helping raise awareness. 

The separation of events into these categories helps us to have a feel for the mood: a paper discussion is fun, exciting, inspires curiosity, and not generally undertaken while a group is rushing to meet a deadline. A ``pair programming\footnote{not generally called this even if it looks like it; grouped with 'expertise exchange' because the reason for these is often explicitly to pull in someone who has additional specialized knowledge. I return to ``pair programming'' as a concept and a term in Chapter 5.}'' session, or a meeting between 2 or more people which involves working closely on a single code-base, has the potential for much more urgency or frustration. This is the more ``problem-solving'' kind of ``event.'' Events like talks and paper discussion are included - they are not centered on code, but given the ubiquity of programmatic means to accomplish a great variety of tasks, issues of trusting or re-implementing code come up in these events and are included.

\subsection{Observing Code Work}

As shown in Table \ref{tab:counts}, I conducted 28 semi-structured interviews during this study; the protocols for these is included in Appendix A. As is typical in semi-structured interviews, most of the interesting questions come as follow-ups. Aside from 28 ``formal'' interviews - scheduled, with an audio-recording device, in a purposely private office - there were countless more casual interactions where I could ask the salient questions. Additionally, toward the end of the study, I asked speculative questions. For example, during a lunch break, I might ask the participant to critique an idea for interactive visualization for debugging. The goal of these probes was to inspire additional reflection, in a concrete way, about a larger issue, like trusting automation. Often I would summarize what I saw that morning in observation, or a small portion of the working insights so far, as a kind of sanity check. Most formal interviews were audio recorded and transcribed, though many insights emerged from the informal lunchtime ``interviews,'' which I transcribed to the best of my ability in observation fieldnotes.

In addition to formal and informal interviews, I conducted 126 hours of observation of code work, the breakdown for which is shown in Table \ref{tab:counts} (304 is the total hour count, and 126 is the total for ``Shadowing''), which excludes interviews, check-ins, and discussion of code in meetings or events. As noted in the beginning of this chapter, ``shadowing'' consisted mostly of quietly co-working in the same space at the scientist being ``shadowed'' and asking periodically what they are stuck on, what solutions they are trying, how they know if it is working or when do they decide to quit and try a different approach, and so on.

The choice of the word ``code,'' rather than ``software,'' "programming," or "scripting" is intentional. ``Code'' refers to the \emph{programmatic instructions} written in the course of software production (and sometimes software use), programming, and scripting, though these are distinct coding activities, which I refer to as different kinds of \emph{code work}, as I categorize and illustrate in Table \ref{fig:codework}.


\ca{explain goliath}

\input{3methods/codework.fig}

\subsection{Biases and Researcher Stance}

One of the 4 groups studied was at the University of British Columbia department of Earth, Ocean, and Atmospheric Sciences, and the other 3 were at the University of Washington School of Oceanography. In the course of collecting data, I travelled a dozen times to Vancouver, BC for observations and interviews spanning a year with a particular focus. As Miles, Huberman, and Salda$\tilde{n}$a point out, one of the sources of bias in a qualitative study is the effect of the research site on the researcher \cite{miles2013qualitative}, such as when the researcher becomes too close to the meanings and values of the informant. This kind of ``going native'' is arguably especially tempting in the study of science \cite{latour2013laboratory}, as both the researcher and the informant share certain aspects of their creative endeavors.  Although it was not intentional, the physical, temporal, and emotional distance between Vancouver and Seattle allowed me to gain unanticipated but welcome perspective an all the study sites. Indeed, Miles, Huberman, and Salda$\tilde{n}$a recommend ``spending time away from the site; spread[ing] out ... site visits'' as a way to avoid this form of bias \cite{miles2013qualitative}. Physically changing location between Seattle and Vancouver, including the multi-hour bus and bike ``commutes,'' inspired almost all of the most interesting insights reported in this document.

I worked to mitigate the effects I might have on the site as a researcher, because they seemed to loom especially over this study. My programming background - and my affiliation with a Computer Science department - allowed me to observe the kinds of \emph{code work} I described in the previous chapter at a fine level of detail, but also occasionally prompted the participants to ask me for an evaluation or assessment. Additionally, I believe that more often statements like ``I should really [be doing this thing] but I'm not'' may have been entirely for my sake, an acknowledgement that their practice deviates from some protocol but not necessarily with any guilt, shame, or anxiety about it. When I encountered these kinds of reactions, I asked further questions, in line with the advice to ``asking [the informant] to be attentive to your influence on the site and its inhabitants'' to avoid the bias of researcher effect on the site \cite{miles2013qualitative}, such as by asking further why they think that they ``should'' or ``should not'' do something, and the reasons for their choice to diverge from the perceived expectation or standard. I would further reassure the informant that my purpose is to understand their programming practices, rather than to judge or evaluate.

In additional to the potential biases in interaction with the informants or with data collection, there are potential biases in analytic approach, which I addressed by seeking feedback from non-computer scientists throughout the data collection and iterative analysis processes. My computer science training offers me  a  vocabulary for describing things and events; it also offers an implicit and abstract set of ideas about what is or is not ``elegant'' or even ``good.'' My aim as a qualitative researcher was to use the knowledge I had to be able to understand what people were working on, but to recognize and set aside - as much as possible - my learned judgments.

Below, I describe in more detail one such example, involving the terms ``bug'' or ``debugging.'' This is possibly one of the least-used words by participants; indeed, I have a strong suspicion that on the few occasions the word was used, it was because I used it in a question without catching myself. Rather than calling things ``bugs,'' most participants said that their code ``is not working'' or ``being weird.'' Rather than ``debugging code,'' they would re-examine the code as well as look over the data (if any) in plain-text or in an overview visualization, or re-do the math (if any) in pencil-and-paper or whiteboard.\footnote{Almost every time my follow-up questions about a bug were not easily answered with a few phrases, the participant grabbed a nearby pencil and paper, or marker at the whiteboard, and sketched something explaining the inter-operating components - invariably not \textit{only} in the code but outside of it, as well.} In an overwhelming majority of cases, the bug had to do with assumptions in parsing and formatting or dates; unit conversion; or a sneaky divide-by-zero error arising from the current coder making assumptions different from the previous coder on the range of some variable(s). 

One way to interpret the apparent lack of calling debugging by its name is to see it as symptom of a problem, and to suggest a solution. These non-expert programmers, without formal training, are not fully appreciating the extent to which the implementation of a mathematical idea, for example, is distinct from that idea: checking the math gets you only part of the way to the bug (this is one of the points made in \cite{heaton2015claims}). The resulting recommendation could involve suggesting that scientists, at workshops like SWC, are encouraged to speak more about code as a separate entity, with its distinct opportunities brittleness - such as by formally acknowledging ``debugging.'' Alternatively, we might recommend doing pair programming with a more experienced software engineer who can guide the inquiry to possible holes in the code.

Another option in this situation would be to acknowledge that it is hardly surprising that people focus on what they are most familiar and comfortable with in problem-solving. Borgman describes a similar effect between the ``science team'' and the ``technology team'' \cite{borgman2012whos}; and Patel et al have written about the reverse effect: the fact that programmers focus more on the code than on the data makes it more difficult for them to train good machine learning models \cite{patel2008examining}.

The difference between the first interpretation and the second is the difference between critical, evaluative assessment and ethnographic study. Another way to look at this difference is to contrast the terms \textit{etic} and \textit{emic}: the first refers to concepts and terminology applied by outsiders to a group of people and their activities; and the second refers to the concepts and terms as they are understood by the group of people themselves. Being surprised by a lack of ``debugging'' is an etic judgment; moving away from ``debugging'' and focusing on the mechanics of problem-solving in the terms of the scientists themselves aims to understand the emic values and meanings.

Software engineering literature  has used literature reviews \cite{heaton2015claims,sletholt2011literature} and well as surveys and interviews \cite{hannay2009scientists,ko2004six}. Work by Adolph, Hall, and Kruchten translates grounded theory methods for software engineering practitioners, and makes a case for it as a valid means for building knowledge \cite{adolph2011using}. Advocacy for deeply qualitative methods in computer science research and system design dates back for for decades (e.g., \cite{friedman1996value,anderson1994representations}). Furthermore, working closely with particular teams to design and validate interventions is a recognized practice in CSCW as well as in SEng (e.g., \cite{young2015re,palyart2011improving} respectively), though  for historical and community constituency reasons, formal qualitative methods are far more common in the former. The use of qualitative methods in this dissertation is not groundbreaking in computer science; the more unique turn that this dissertation takes is the simultaneous recognition of the idealized software engineering concepts (etic) at the same time as aiming to contextualize and describe the participant's own meanings and values on the relevant subjects (emic).