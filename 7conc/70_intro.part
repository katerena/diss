\chapter{Evaluation of Intervention Outcomes}

The expectation of an experimental software product to be the primary solution to a major problem does a disservice to both creators and users. This chapter explores alternatives to the narrative of software interventions that paint a promise, identify an associated obstacle, and propose a solution in the form of a software intervention. Figure \ref{fig:ch7toc} illustrates the different intervention contexts. Each of the sections maps to an intervention in a different context, and concludes with a take-away that follows from the conceptual framework developed in the prior chapters. These are summarized in the final section as encouraging intentional recognition of ways in which groups make progress even when specific technological interventions do not work out.

An over-zealous emphasis on the novelty and centrality of particular technical challenges (e.g., big data) is both historically erroneous in painting scientists as passive recipients of qualitative shifts in available technologies, despite the critical role of these very ``end-users'' in having brought about these shifts in the first place, over much larger time scales than some technology-innovation-centric abstracts would suggest. Computer-science study of scientific work sometimes describes the 
current state of programming, code, data and analysis infrastructure, and software production in general (in the scientific domain) as somewhere between unsustainably frustrating and untenably chaotic. Recent technological advances have changed the set of available tools, accepted practices, and institutional resources. However, what is `recent?' A mid-2000s  article speaks of a `data avalanche' in environmental
sciences, calling for a ``computer science agenda spanning databases, visualization, workflows'' \cite{howe2008end}.
Speaking about the early 1950s, Edwards writes in A Vast Machine that ``climatologists ... 
faced not only an explosion of data but also a plethora of alternatives
for coping with it'' \cite{edwards2010vast}. These are by far not the only paragraphs
on the topic that paint a picture of the scientist coping with 
``cataclysmic'' amounts of data, requiring technological innovation. 

\begin{figure}
    \centering
    \includegraphics[page=1,width=1\textwidth]{7conc/Ch7_TOC.pdf}
    \caption[Three different ways in which computer science intervenes in domain-science code work.]{\textbf{Three different ways in which computer science intervenes in domain-science code work.} (a) By working on a particular project that is part of a larger infrastructural and collaborative effort and which ultimately aims to support many scientists in their study of many different phenomena; (b) by organizing educational interventions, like workshops, tutorials, and mentorship programs to support scientists who code in developing programming skills, who are then mostly on their own to apply those skills to particular projects and research questions of interest; and (c) by collaborating closely with a group pursuing particular research questions, and building tools or intervention specifically tailored to that group or context. ``CS/SE'' stands for ``Computer Science / Software Engineering'' but extends other computing-affiliated researchers.}
    \label{fig:ch7toc}
\end{figure}

This narrative does a disservice to both computer scientists and domain scientist working on cutting-edge experimental software. It fails to use the rich resources available from the domain scientists side, and it fails to provide room to recognize the successes of projects that do not result in a complete achievement of the technical goals. In a landscape that benefits from experimentation and trial-and-error, I argue that there are more productive alternatives. Each section highlights an alternative, and the concluding section synthesizes these into an evaluative stance that emphasizes social and skill outcomes for building and sustaining constructive momentum.

First, this view \textbf{underutilizes the technical, social, and cognitive resources of domain scientists.} In the prior chapters, I described the working environment of the oceanographers as including not only particular tools (iPyNB, MATLAB, GitHub), but also social resources (``documentation'' over email with colleagues) and cognitive resources (extensive visualization vocabulary used for validation and verification; see Section 4.3). The negative outcome of this under-utilization is the focus of Section 7.1, in the context of an experimental component of a large infrastructural effort that corresponds to (a) in Figure ~\ref{fig:ch7toc}.

Second, this narrative \textbf{suggests a very limited success expectations that do not include other forms of impact.} Such an approach of finding problems, or opportunities, and building technological interventions to address them leaves few options for evaluation: there is one way to succeed, and many ways to ``fail.'' The ``failure'' may still offer valuable insights, but the evolution of the problem or the (informative) shortcomings of the solution become defensive arguments. The conceptual framework offered thus far in this dissertation is intended to complement this. Any new software tool or programming skill is adopted into - and adapts - a rich working environment that includes technological, social, and cognitive resources. Even if the adoption is not sustained over time, the attempt itself can translate into an \textit{expanded imagination of possibilities}, if not actual change in practice. Although the sense of elegance of a scientist who codes may not coincide with the sense of elegance nurtured in computer science training and professional communities, it nevertheless informs reactions like excitement and boredom that affect decision making outside of interpersonal or utilitarian concerns (see Chapter 6). Section 7.2 and 7.3 offers stories from my fieldwork that demonstrate more inclusive articulation of goals and recognition of successes.

Third, the promise-obstacle-solution narrative in the experimental, transformative code work context in science \textbf{enables maintaining misaligned imaginations}. This, too, is the subject of Section 7.1, where there is a gap between how different \textit{communities of practice} involved in a major interdisciplinary collaboration assign value to different components of the system as crucial to get right, versus incidental. In this following section, I illustrate my claims with a reading of a blog post and a whitepaper about the Ocean Observatories Initiative (OOI). The blog post, written by a prominent oceanographer, criticises the unavailability of the data generated by an impressive hardware infrastructure simply because an unnecessarily-complex software component is behind schedule. the whitepaper, written by the technologists behind this ``cyberinfrastructure'' component, stress the that the integration only possible through the completion of this component is strictly necessary for effective use of the data. Through integration allows data operations that are otherwise not possible, there is still a notable difference in what the two documents hold as their vision of the \textit{perfect world}.