\section{The Perfect World}

A software advisor in the CustomInstrument-Lab team comments: ``in a perfect world, there would a satellite for data transfer during cruise, but this is not that world, so data is transferred once in Hawaii.'' The software advisor of the BioGeoChem-Model group comments, during almost every explanation of the current working environment, that it is ``not ideal'' while providing a historic or domain-specific reason for it being so.  Acknowledging imperfection in the current working environment can be seen as a way of comparing it to an imagined ``perfect'' world (what-could-be), and found to be lacking in a way that informs learning. Drawing attention to the imperfections is not quite defensiveness, but ubiquitous comments like describing something as ``kind of terrible but I still think it's really useful.'' The word ``perfect'' was chosen because of its relative ubiquity in data, along with ``ideal,'' used similarly in saying something is ``not ideal'' or in saying that ``it would be ideal to ...''.  In other words, the ``perfect world'' is that world which you would re-build, in place of the current, ``not ideal'' reality, if only time were to stop, and you to have the opportunity to apply everything you have learned from the first time around. In this section, I elaborate on the uses of the phrase ``in a perfect world,'' and the descriptor ``not ideal'' when describing the current solution. First, I introduce the ``collective imagination of a perfect world'' as a social practice. Second, I describe some of the features of the ``perfect world'' when it pertains to code, which are illustrated in the Figure \ref{fig:perrrfect}.

\begin{figure}[tbhp]
    \centering
    \includegraphics[page=1,width=1\textwidth]{4teams/perrrfectv2.pdf}
    \caption[Desirable qualities of code in the \textit{perfect world}, as they influence audience access.]{\textbf{Desirable qualities of code in the \textit{perfect world}, as they influence audience access.} In the perfect world, denoted in this illustration with a star has the overlapping qualities of being usable, extensible, and understandable by everyone. Something can be ``understandable'' without being usable or extensible if it serves to document or guide; for example, code on StackOverflow sometimes cannot be run, but is still ``understandable'' enough to be useful. Code cannot be both ``usable'' (in an end-user sense) and ``extensible'' (by a motivated coder) without also being understandable. Typically, scientific code resides at the top of this chart: it can be made to have any of the properties, but only to motivated uses through extensive, contextualized guidance. Moving downward in this illustration toward this star expands the potential audience, reduces the amount of ``hand-holding'' (see Section 7.3), and comes closer to the perfect world.}
    \label{fig:perrrfect}
\end{figure}

Figure \ref{fig:perrrfect} includes not only the desirable characteristics of code in the perfect world, but also ways in which they can affect different audiences. All code work has \textit{some} audience, at the very least the (1) future self of the programmer, or the current skeptical self of the programmer. Additionally, the potential audience of code work includes (2) lab members, colleagues, and others who have a relatively high familiarity with the work and are exposed to intermediate version, help with various challenges; (3A) colleagues working across time: from prior projects or workplaces, or hypothetical future colleagues or students; (3B) colleagues working across space: from other institutions or labs, where there is mutual interest in sharing data (in either or both directions). Because the working environment - as it is described in the prior Section 4.2 - is a complex and personal collection of inter-dependent, highly contextual parts that undergo continuous change, \textit{all} of these audiences (including the future self) require additional translation, integration, and verification across distinct environments. 

Comparisons to the ``perfect world'' constitute a future-oriented way to identify the perceived deficiencies of the working environment or its products. These are in contrast to past-oriented critiques: the explanation of something for ``historical reasons,'' or for reasons that involve identifying a long chain of stakeholders or programmers, their contexts, and an explicit recognition of how this has resulted in the current, ``not ideal'' situation. The \emph{expertise} necessary for the ``perfect world'' comes through in the precision of perfect-world statements. The precision is informed by the experience of specific disappointments: it is a retroactive, ``hindsight is 20/20'' kind of expertise. The difficulty of attaining perfection is that the path forward remains unclear despite lessons learned in the past. When articulates as a ``specification,'' the perfect world is imagined as something that could realistically be implemented, but only if there is someone whose primary role includes doing so; though this sentiment is expressed by people whose roles only peripherally involve programming as well as those who are full-time programmers focusing on technology. Regardless of how attainable the perfect world is said to be, it is not, in actuality, attained. It changes in response to what \emph{is} attained, and every role is filled by a person who feels stretched to the limit of their resources.

I distinguish the imagination of the ``perfect world,'' which requires some expertise and familiarity, from the imagination of the ``silver bullet.'' At one point, the Omics-Lab group decides to try out Tableau, with a great deal of initial excitement, followed by a bit of disappointment: this is not the solution to everything, though it does have its advantages in particular contexts. When a post-doc first tries it out, the PI and the group are excited, which shows in an email exchange about the lab meeting (it goes from the PI asking if the meeting is necessary and whether it ought be rescheduled, the post-doc replies with an excited message about having charts to show, and the meeting is enthusiastically back on).

The PI of the group was direct and explicit about her anxiety over new tools/skills (and perhaps not using them enough, or not knowing the options well enough) and wanting to do something, and thus seizing on the opportunity of my showing something very simple to the post-doc. It is not mind-blowing and what she makes with it is not perfect. Later, another group member, a graduate student, uses Tableau to make some plots, switches to R because Tableau does not offer enough control over formatting, and produces ``beautiful'' charts that are the subject of great excitement, as the data is convincing and expressive.

Excitement about the initial attempt to use Tableau was revealed through emails prior to the weekly meeting, in which the post-doc wrote that she had Tableau chart to show, and to which the PI replied with enthusiasm. The exchange involved a exclamation points and happened in the morning before the noon-time meeting; not surprising for a friendly group with robust meeting scheduling practices, but unusual relative to other meetings I had attended\footnote{It's possible some of this excitement was so my sake, but I strongly doubt all of it was.}. Excitement about the second ``beautiful chart,'' about which I wrote previously, was expressed in a mention in the stairwell from the PI that I should ``ask Alice about her beautiful chart! She made it in R!''

When I follow up with Alice, a senior graduate student, she tells me, almost apologetically, that Tableau did not offer the fine-grained control over visual aspects of the chart that would be necessary for it to be a publication-ready visual artifact. However, as she explains to me the different-size and different-colored disks arranged to follow the path of the cruise on a map of the region of interest, which she made in R, and then a table of pie-charts, the visual vocabulary looks less like what I have seen in oceanography talks and more like the visual vocabulary of Tableau. In a way, it had an impact; but, even if it did not, it needed to be tried in order to be rejected for being insufficiently controllable.

%Read the vignette… https://docs.google.com/document/d/1kG5fvBjemCcqoZSWd-ixSa5-oGVbdtscCOfE8Z5dVv0/edit

On a different occasion, the software advisor of the RegionalNowcast-Model explains to me how a lot of their python scripts use a large model software with many constituent packages and many diverse users internationally. He describes to me running a single run with a single command and in ``a perfect world,'' this command would deal with creating symbolic links, creating directories, adding jobs to the queue manager in the computing resource. Output in this world is one output file per variable per process, e.g. on order of 2K files that need to be re-integrated into a more coherent result. Several months later, he unveils such a perfect-world command - to the students and post-docs, four people total, who comprise its current users. The demo is riddled with mentions of the many ways in which the command would be improved in the perfect world 2.0. The programming and documentation work that went into this project had as its audience the current researchers, as well as the unknown but expected and desired new graduate-student additions to the group. The previous perfect world of having a single command had expanded to having not only more features, but also to a wider and more diverse audience.
%OCND_FN150826_KK

A post-doc in the RegionalNowcast-Model group explains to a junior student, as she sits with her, doing a backseat-advising kind of pair-programming: ``In an ideal world, our code runs perfectly and the user never looks at our code, but that’s not what happens in practice. In practice, there is always some case we haven’t accounted for. So, first thing - will someone be able to look at this code and understand? I like all the docstrings\footnote{In python, a way of commenting in a function that allows that comment to appear as API documentation later when the functions are used/ auto-completed as the programmer types.} [she points out what the student has done well before offering a suggestion] but would be nice to include some inline comments.'' In this case, docstrings will provide verbose and descriptions on space/tab-complete for users of the Python module, whereas inline comments will be helpful for someone who is examining the code itself (rather than only using it as a working black box). Both of these are ways in which the researchers are enabling the code to communicate with its various audiences.

Talking about collecting data in the sea, a member of the BioGeoChem-Model group is talking to a colleague whose data he is considering using for a very specific research task that involves making a comparison between their datasets. They are discussing measurements made by profilers, and there is some discussion whether the profilers follow eddies or not. The colleague explains that because the  profilers go so deep, they are basically random with respect to the eddies, so the post-doc exclaims: ``that's perfect!!'' because it means the sample can be comparable. More often than not, the in-depth discussion of data applicability yields the verdict of a less-than-perfect-world. A graduate student in the Omics-Lab comments, regarding using the mass spec for chemical analysis: ``Then in a perfect world, you have standards for all of the compounds that you're looking for that you can compare with and say this is how much standard I had, and it gave me this big of a peak. This is how much of the peak was my sample. Then I know how much was in my sample.''

Because it is situated relative to an audience - even if it is just the future self, which tends to, upon questioning, also imply hypothetical future collaborators - the perfect world is \textit{collectively imagined} in that it reflects values that are shared among the people to whom the complaint or the dream is articulated. In the CustomInstrument-Lab group, semi-joking frustration also reveals an exasperation with computer scientists who try to hide the inner workings: ``we are \textit{scientists}! looking inside and figuring it out is what we do!''  In the BioGeoChem-Model team, I also heard joking about how hard it is to get the software advisor to explain what he does: ``he doesn't like talking, just fixes all our problems super fast.'' In both cases, there is a need to understand not only at the practical level, but also in terms of seeking understanding as a core aspect of the craft of doing science.

Understanding is at the heart of the perfect world, and can be in some ways incompatible with usability. Ryan tells me, over a lunchtime discussion, of the difference between 2 major programs, also reflecting the more-control-is-better but in a different tune. He compares and contrasts the two programs by saying that both are ``Goliath programs'' and one is more ``conservative'' (in terms of including new packages) which makes it generally more ``reliable.'' Furthermore, neither is especially ``user-friendly'' because the ``programmers don't want people to get too complacent.'' He tells me about the Goliath packages in context of having asked me to comment about my research so far\footnote{For this situation, my intention was to reply truthfully about the study but also not give away ``too much.'' Especially in this group, which was wonderfully friendly and inviting to me, so my main concern was that if I gave too much detail of what I was expecting or hoping to find, I would be shown whatever it sounded like I wanted to see.}; so I commented on how my study was less about collaboration between oceanographers and computer scientists, with the scientists primarily playing the role of the user, and more about code work that is done \emph{by} oceanographers, which includes working with modeling code as well as small scripts\footnote{these, as opposed to modeling, are grouped under ``Kleenex code'' by Heaton and Carver - hence the title of the section \cite{heaton2015claims}}. Though my intention was to make a neutral statement, both Ryan and Mary corrected me with one of the many ``not a lot of people do what we do'' that I would encounter through the course of this research. They told me that great deal of researchers - those not included in this study with an inclusion criterion of active engagement with programming education or collaboration with computer scientists - used such large ``Goliath'' programs. Though upon explanation, it was revealed that this, too, would require at least a little ``Kleenex'' code as well.

There is, however, a great deal of code that is neither ``kleenex'' nor ``library'' (nor the other categories suggested in Fig. \ref{fig:codework}). In a way, it is not even code, but a highly particular ``figure caption'' or ``experiment annotation.'' However, it would not exist without some other code practice; the scientist who has written nothing but figure code in a few months is likely to have written a considerable portion (or totality) of an analysis or pre-processing script or program in the past, or likely to write it in the future. On the other hand, the ``kleenex'' code for analysis, processing, or figure-generation can be both a cognitive resource (a detailed lab-notebook protocol of what was done) and a technical resource (reusable). The cognitive resource is effectively persistent when it is readable by scientific peers (or by the future self of the original researcher) enough to be able to replicate the action; it less important that it `work''and more important that it remain accessible over time. The technical resource, on the other hand, is effectively persistent when others are able to run it, and to modify it as necessary. It still must ``make sense,'' but also ``work.''

Although this type of ``code to document a visualization'' is useful over time and a core part of scientific work, it elicits  off-hand comments from some informants that they ``shouldn't be copy-pasting.'' This reflects a larger ``best practice rule of thumb'' that the moment that a programmer begins to copy paste code, is the moment she should re-evaluate her actions and switch to a more modular design. In the context of ``traditional programming,'' ``persistent code'' is synonymous with ``implementation that persists overtime while its context of execution changes,'' which means this code is subject to degradation relative to its slowly-evolving specification. In the context of scientific programming, ``persistent code'' can \emph{also} be code that is subject to no changes in execution context.

The tooling challenge here is to capture this back and forth in a way that can be accessible later. On several occasions, the chart or data of interest were produced by those retired or dead, therefore unable to engage in the back and forth, which limited the interpretation of what was in the papers. A record of questions asked by other researchers and answers as as useful as the answers; StackOverflow, after all, addresses a vital and universal need. Nevertheless, doubtless other researchers in the past had asked some clarifying questions, and access to those may prove a useful history.

Another distinct, but also pervasive, social interaction over visualizations is again focused on the individual corralling the data and model run results from others in order to generate a complex visualization with many comparisons. Even if the data or results in question are published, there is still an email chain to discuss those assumptions and consideration relevant to the research question but left out of the manuscript. At least as much time is spent procuring this documentation - in the form of emails and Skype calls - as it is on discussing which details to leave out of the manuscript or the supplement at the time of publication. Asking questions about charts and data is a kind of expert knowledge transfer, as well as a daily recognition of ownership, arising from an expectation of a particular investigator being the most well-versed person in their particular area.

Code written in the ``visualization volley'' is itself the documentation. In a working environment that spans cognitive and social, not only technical resources, this constitutes a \textit{cognitive} one. This code is stand-alone, persistent, and essentially non-modularizable: the goal is to ``freeze'' the visualization in time until it requires amendment in response to reviewers. Visualizations that are relatively more complex (in terms of including multiple sources of data or representing a multi-part analysis), even if not necessarily intended for publication, still remain the best workable summary of the experiment. They encode not only the question and answer, but the provenance. The worst case is the folder full of figures whose origin is unknown, especially if one of them looks intriguing. The best case has been one-off iPython notebooks that have gone through a process of being ``cleaned up'' and annotated with ``figure captions.''


Code that ``documents'' a visualization is useful over time and a core part of scientific work, it elicits a low-priority concern from scientists, mentioned off-hand, that they ``shouldn't be copy-pasting.'' This reflects a larger ``best practice rule of thumb'' that the moment that a programmer begins to copy paste code, is the moment she should re-evaluate her actions and switch to a more modular design. In the context of ``traditional programming,'' ``persistent code'' is synonymous with ``implementation that persists overtime while its context of execution changes,'' which means this code is subject to degradation relative to its slowly-evolving specification. In the context of scientific programming, ``persistent code'' can \emph{also} be code that is subject to no changes in execution context. There are no conflicting goals if we separate the instances where code is a technical resource from those where it is a cognitive resource.

Though  code to document a visualization is not subject to internal degradation through shifting execution context, it is still subject to external degradation of code: the libraries it depends on, the compiler that reduces it to machine code, the programming language specification may all change in the time between when the ``figure caption'' was frozen and the time it needs to be unfrozen. When faced with this variant of ``bit rot,'' Mallory noted that at first it was frustrating to have to re-create all the charts (previously in MATLAB) in python, but this was a preferable time investment over updating the .m scripts, because she had been meaning to learn to make publication-level charts in python. The need to spend several days polishing python code was, for her, enjoyable and rewarding: it was relatively straightforward, and it resulted in a skill that she was happy to have honed. She used playful, constitutive\footnote{not mutually exclusive} code work in order to make a technical resource with longer-lived usefulness.

In addition to discussion over a single artifact produced and managed by an individual, some dynamics call for a visualization volley between a small number of individuals, who all have stakes in the visual artifact and contribute to its formation. Consistent with the prior case, the potential end result for a chart is inclusion in a manuscript as a figure, so retaining fine control over visual elements like colors, styles, and fonts remains paramount. However, if multiple people edit the same visualization, they must balance the need for control with the challenge of using low-level scripting in MATLAB. Lastly, recognizing the different roles that code might play - as communication, for example, not for automation, as in StackOverflow - can help distinguish \textit{which} code ought to be subject to which evaluation relative to the best practices of the perfect world.

\tk Although I spend most of this section talking about the vision of the perfect world in terms of the vision of how programming can change work practices, the collective imagination is not exclusive to code work. As an example of a collective vision of a perfect world which involves code work but which is not, at its core, programming-oriented, consider \textit{reanalysis}, on which Edwards spends a chapter in \textit{A Vast Machine} which is on climate modeling that includes ocean general circulation models. This is an algorithmic approach to combine historical and model data, which ``has been disappointing ... as a replacement for traditional climate data'' \cite[Chapter 12]{edwards2010vast}. Aside from ``significant differences... between the major reanalyses as well as the reanalyses and major traditional climate data sets,'' the issue with reanalysis is that although the results agree reasonably well for ``variables constrained directly by observations, such as temperature,'' not for the derived variables: ``for example, reanalysis models do not yet correctly balance precipitation and evaporation over land and oceans, whose total quantity should be conserved. This affects their calculations of rainfall distribution, a climate variable that is extremely important to human populations and to natural ecosystems.'' My inclusion of this passage is not intended to highlight reanalysis and its relative successes alone, but to highlight the extent to which particular technologies (modeling) allow actions that are not merely hard but impossible (calculating derived variables over long time-spans) so long as there are means for verification (conservation laws and observational data constraints). In the case of reanalysis, the hope of the technique did not match its ``disappointing'' reality despite the fact that ``the do-over has already been done over several times,'' with the approach being ``hugely expensive, requiring many weeks of supercomputer time as well as ongoing data cleanup.''